{"cells":[{"cell_type":"markdown","metadata":{"id":"p9vEDloTOkQ1"},"source":["# 3.allow user to select reference corpus\n","\n","#Feature3"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2652,"status":"ok","timestamp":1689907909136,"user":{"displayName":"米田武士","userId":"14881788265874075884"},"user_tz":-540},"id":"ln46tEDTOQt6","outputId":"b415042c-13c9-4b7b-827e-83324cf07c82"},"outputs":[{"name":"stdout","output_type":"stream","text":["SUCCESS: You chose: 2 Enron corpus\n"]}],"source":["dataset1 = \"Brown corpus\"\n","dataset2 = \"Enron corpus\"\n","while 1:\n","    selected_dataset = input(f'Select Courpus you want to use: \\n 1: {dataset1} 2: {dataset2}\\n')\n","    selected_dataset = int(selected_dataset)\n","    if(selected_dataset == 1 or selected_dataset ==2):\n","        exec_command = f\"print(f'SUCCESS: You chose: \" + str(selected_dataset) + \" \" + \"{dataset\" + str(selected_dataset) + \"}')\"\n","        exec(exec_command)\n","        break\n","    print('Please input de cimal number\\n')\n","    try:\n","        selected_dataset = int(selected_dataset)\n","    except:\n","        print('Please input decimal number\\n')\n"]},{"cell_type":"markdown","metadata":{},"source":["## Remove NaN and Change Enron Documents into one Document"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"TlXI0jWwYSVb"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","pd.set_option('display.max_rows', None)\n","pd.set_option('display.max_columns', None)\n","def rmNan(df):\n","    #remove nan\n","    for i, msg in enumerate(df['message']):\n","        # print(i, msg)\n","        if msg is np.nan:\n","            df = df.drop(i)\n","    for i, msg in enumerate(df['message']):\n","        if msg is np.nan:\n","            print(i, msg) \n","\n","    #merge documents into one\n","    str_all_document=''\n","    for index, record in df.iterrows():\n","        str_all_document = str_all_document + str(record[1])\n","    del df\n","    \n","    return pd.DataFrame({\"author\":[\"ENRON DATASET\"],\n","                        \"message\": [str_all_document]})\n","            "]},{"cell_type":"markdown","metadata":{"id":"zOvUoP9sRtVm"},"source":["## Make DataFrame, Q, K1, K2, and Enron or Brown Datasets"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1689908970311,"user":{"displayName":"米田武士","userId":"14881788265874075884"},"user_tz":-540},"id":"lKqY6J0pdYTM"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import glob\n","def makeDataset(datasetnum = 0):\n","    data = \"\"\n","    if(datasetnum == 2):\n","        df = pd.read_csv(\"preprocessed_enron.csv\")\n","        df = rmNan(df)\n","    elif(datasetnum== 1):\n","        files = glob.glob(\"./Brown/*\")\n","        for file in files:\n","            f = open(file, 'r')\n","            data = f.read() + ' ' + data\n","            f.close()\n","            break\n","        df = pd.DataFrame({\"author\":[\"BROWN DATASET\"],\n","                          \"message\": [data]})\n","        print(data)\n","    return df.reset_index(drop=True)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["#Make df_Q dataset \n","f = open('Q_dataset.txt', 'r')\n","data = f.read()\n","f.close()\n","df_Q = pd.DataFrame({\"author\":[\"Q DATASET\"],\n","                    \"message\": [data]})\n","                    "]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["#Make df_K1 dataset \n","f = open('K1_dataset.txt', 'r')\n","data = f.read()\n","f.close()\n","df_K1 = pd.DataFrame({\"author\":[\"K1 DATASET\"],\n","                    \"message\": [data]})\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["#Make df_K2 dataset \n","f = open('K2_dataset.txt', 'r')\n","data = f.read()\n","f.close()\n","df_K2 = pd.DataFrame({\"author\":[\"K2 DATASET\"],\n","                    \"message\": [data]})"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"ename":"KeyError","evalue":"'message'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","File \u001b[0;32m~/workspace/Git_Projects/expert-system/.venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n","File \u001b[0;32m~/workspace/Git_Projects/expert-system/.venv/lib/python3.10/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","File \u001b[0;32m~/workspace/Git_Projects/expert-system/.venv/lib/python3.10/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'message'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#Make df_ref dataset \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_ref \u001b[39m=\u001b[39m makeDataset(selected_dataset)\n","Cell \u001b[0;32mIn[3], line 8\u001b[0m, in \u001b[0;36mmakeDataset\u001b[0;34m(datasetnum)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mif\u001b[39;00m(datasetnum \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m):\n\u001b[1;32m      7\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39mpreprocessed_enron.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m     df \u001b[39m=\u001b[39m rmNan(df)\n\u001b[1;32m      9\u001b[0m \u001b[39melif\u001b[39;00m(datasetnum\u001b[39m==\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m     10\u001b[0m     files \u001b[39m=\u001b[39m glob\u001b[39m.\u001b[39mglob(\u001b[39m\"\u001b[39m\u001b[39m./Brown/*\u001b[39m\u001b[39m\"\u001b[39m)\n","Cell \u001b[0;32mIn[2], line 7\u001b[0m, in \u001b[0;36mrmNan\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrmNan\u001b[39m(df):\n\u001b[1;32m      6\u001b[0m     \u001b[39m#remove nan\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     \u001b[39mfor\u001b[39;00m i, msg \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(df[\u001b[39m'\u001b[39;49m\u001b[39mmessage\u001b[39;49m\u001b[39m'\u001b[39;49m]):\n\u001b[1;32m      8\u001b[0m         \u001b[39m# print(i, msg)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m         \u001b[39mif\u001b[39;00m msg \u001b[39mis\u001b[39;00m np\u001b[39m.\u001b[39mnan:\n\u001b[1;32m     10\u001b[0m             df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mdrop(i)\n","File \u001b[0;32m~/workspace/Git_Projects/expert-system/.venv/lib/python3.10/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3762\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n","File \u001b[0;32m~/workspace/Git_Projects/expert-system/.venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n","\u001b[0;31mKeyError\u001b[0m: 'message'"]}],"source":["#Make df_ref dataset \n","df_ref = makeDataset(selected_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = pd.concat([df_Q,df_K1, df_K2, df_ref])\n","df = df.reset_index(drop=True)\n","del df_Q, df_K1,df_K2,df_ref"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>author</th>\n","      <th>message</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Q DATASET</td>\n","      <td>\\n\\nHowever, there are frequent situations whe...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>K1 DATASET</td>\n","      <td>Download\\n\\nSource\\n\\nPDF\\nActions\\n   Copy Pr...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>K2 DATASET</td>\n","      <td>\\n\\nWith the rapid growth of the information c...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>BROWN DATASET</td>\n","      <td>Appointment of William S. Pfaff Jr. , 41 , as ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          author                                            message\n","0      Q DATASET  \\n\\nHowever, there are frequent situations whe...\n","1     K1 DATASET  Download\\n\\nSource\\n\\nPDF\\nActions\\n   Copy Pr...\n","2     K2 DATASET  \\n\\nWith the rapid growth of the information c...\n","3  BROWN DATASET  Appointment of William S. Pfaff Jr. , 41 , as ..."]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["df"]},{"cell_type":"markdown","metadata":{},"source":["# 1. count, list and order the frequency of words \n","\n","#Feature1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vaALdJiudYTX"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","def tokenizeFunc(documents):\n","    # documents = df['message'].tolist()\n","    tf_vectorizer = CountVectorizer()\n","    tf_vectors = tf_vectorizer.fit_transform(documents)         # word frequency list\n","    return tf_vectors, tf_vectorizer\n","# for i, msg in enumerate(df['message']):\n","#     # print(i, msg)\n","#     if msg is np.nan:\n","#         print(i, msg)\n","#tfidf_vectorizer = TfidfVectorizer()\n","#tfidf_vectors = tfidf_vectorizer.fit_transform(documents) # keyword frequency list\n","# tf_vectorizer = CountVectorizer()\n","# tf_vectors = tf_vectorizer.fit_transform(documents)         # word frequency list\n","# del tf_vectorizer\n","# del tfidf_vectorizer# データ分割r"]},{"cell_type":"markdown","metadata":{},"source":["### Make All datasets a list and make tf vector and tf vectrizer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["[\"\\n\\nHowever, there are frequent situations when the planning parameters can only be partially defined, the perfect timing or exact location are unknown, and the decision frame is rather fuzzy. For example, we know or are observing an interesting location which is worth visiting, but could not decide in advance on a suitable time frame for such a visit (such as a better season) and a list of possible side activities that would extend a user’s experience significantly (such as visiting exhibitions or museums).\\n\\nEven for the scenarios of standard meeting schedulers aimed at managing the event time, meeting rooms, discussion topics, and lists of attendees; there is space for less decided situations. For example, let's consider the planning process for attending an academic conference. Depending on the conducted research, the authors could consider a range of appropriate conferences, while the exact venue may be unknown in advance. Nevertheless, reminders on the approaching deadlines may be helpful for a team lead, though actually it does not necessarily mean that the paper will be submitted.  \\n\\nThe remaining text is organized as follows. \\nSection~\\\\ref{sec:related} briefly describes a number of related studies, which are helpful for  better positioning of our current work.  Section~\\\\ref{sec:concept} introduces the concept of soft planning and draws on the examples of possible scenarios that can be targeted by the system in focus. Section~\\\\ref{sec:pilots} sums up the lessons learned while developing the components partially implementing the requirements defined at the stage of project conceptualization. Section~\\\\ref{sec:conclusion} concludes the paper and outlines the aspects to be considered in future work. \\n\\n\\\\section{Related Studies}\\n\\\\label{sec:related}\\n\\nIn our earlier works, we introduced a concept of \\\\textit{soft planning} being a metaphor for personalized situational planning scenarios which are (1) not particularly certain, (2)\\nmay be implemented when some suitable situation\\ntakes place (e.g. time and/or locations are not fixed in advance), and (3) in principle, might not be implemented at all~\\\\cite{tang2019ontological,tang2021managing}.\\n\\nPresent-day mobile devices form an ecosystem for the personalization of digital services they provide. In part, support for different kinds of notifications is one of the key features of present-day smart devices~\\\\cite{weber2018snooze}; smart device notifications are no longer just situational alerts, they become an integral proactive component of mobile services\\nengaging with their users~\\\\cite{pielot2018dismissed, poguntke2020notimodes} or enabling them to manage their activity-related choices, for example, through the check-in mechanisms used in social networks~\\\\cite{hasan2013understanding}. Such proactive services help to overcome the limitations of human decision-making conditioned by the naturally limited capabilities of individuals to memorize, store or organize information~\\\\cite{march1994primer,polivc2009decision}. Notifications give the common approach to link the current context to a situation which might be distant in time or/and location, while this context, in turn, can be ``pre-programmed'' by the user as a description of a deferred action to be implemented later. \\n\\n\\n\\nHere are some model scenarios illustrating the concept of soft planning:\\n\\nMobile Notification Service for\\nLocation-based Situational Planning:\\nPilot Project and Lessons Learned\\n\\nAbstract. Digital transformation suggests a high degree of automation while man-\\naging social behavior and planning individual or collaborative activities. Most\\nscheduling and planning solutions are based on time, task and resource manage-\\nment. In this work, we consider the scenarios of so-called “soft” planning, which\\nassumes the exact timing of activities might be unknown and dependent on flex-\\nible conditions. We conceptualize the project with the help of such instruments\\nas common-sense ontology, domain-specific ontology, software requirements visu-\\nalization, and visual statechart formalism. This particular contribution focuses on\\nthe challenges of developing a mobile notification service for managing activities\\n“pre-programmed” by the user, in which notifications are issued if the user enters\\na location suitable for implementing the desired deferred action. The suggested\\nontology-based model does not assume using or improving formal optimal time or\\ntask scheduling, but suggests an approach for informal practical computer-assisted\\ndecision-making involving typical scenarios appearing in everyday life. We piloted\\na number of prototypes for location-based user-oriented reminder setup and notifi-\\ncation management, partially fitting the requirements and major scenarios of a soft\\nplanning system. Based on the experiments with the developed prototype apps for\\nAndroid, we elicit a number of important aspects of further work towards achiev-\\ning the location-based situational planning and notification management solutions\\nadopted for practical use.\\nKeywords. time management, notification, geofence, mobile app, software modeling\\n1. Introduction\\nTechnological support for different types of social activity planning is one of many as-\\npects of human life digitalization. We know many software solutions and smartphone-\\nbased implementation addressing time, budget, or location management, such as\\ncomputer-assisted time, budget, activity or project scheduling [1,2,3,4], event-based so-\\ncial activity planning [5,6], or travel planning [7,8,9,10]. Most practical solutions imple-\\nment the determined scenarios when the constraints and conditions (goals, time, location,\\nactivities, and participants) are known in advance. For example, planning a meeting usu-\\nally assumes setting time and location information, sending confirmations to the involved\\n1Corresponding Author: Evgeny Pyshkin, University of Aizu, Tsuruga Ikki-machi, Aizu-Wakamatsu\\n9658580, Japan; E-mail: pyshe@u-aizu.ac.jp\\nparticipants, etc. There is also a growing interest in developing personalized systems\\nextending user experience based on using social network data and personal user profiles,\\nwhich are often linked to location data as well [11,12,13], for example, in numerous\\nsolutions for travelers [14,15,16,17].\\nHowever, there are frequent situations when the planning parameters can only be\\npartially defined, the perfect timing or exact location are unknown, and the decision\\nframe is rather fuzzy. For example, we know or are observing an interesting location\\nwhich is worth visiting, but could not decide in advance on a suitable time frame for such\\na visit (such as a better season) and a list of possible side activities that would extend a\\nuser’s experience significantly (such as visiting exhibitions or museums).\\nEven for the scenarios of standard meeting schedulers aimed at managing the event\\ntime, meeting rooms, discussion topics, and lists of attendees; there is space for less\\ndecided situations. For example, let’s consider the planning process for attending an\\nacademic conference. Depending on the conducted research, the authors could consider\\na range of appropriate conferences, while the exact venue may be unknown in advance.\\nNevertheless, reminders on the approaching deadlines may be helpful for a team lead,\\nthough actually it does not necessarily mean that the paper will be submitted.\\nThe remaining text is organized as follows. Section 2 briefly describes a number of\\nrelated studies, which are helpful for better positioning of our current work. Section 3\\nintroduces the concept of soft planning and draws on the examples of possible scenarios\\nthat can be targeted by the system in focus. Section 4 sums up the lessons learned while\\ndeveloping the components partially implementing the requirements defined at the stage\\nof project conceptualization. Section 5 concludes the paper and outlines the aspects to be\\nconsidered in future work.\\n2. Related Studies\\nIn our earlier works, we introduced a concept of soft planning being a metaphor for\\npersonalized situational planning scenarios which are (1) not particularly certain, (2) may\\nbe implemented when some suitable situation takes place (e.g. time and/or locations are\\nnot fixed in advance), and (3) in principle, might not be implemented at all [18,19].\\nPresent-day mobile devices form an ecosystem for the personalization of digital ser-\\nvices they provide. In part, support for different kinds of notifications is one of the key\\nfeatures of present-day smart devices [20]; smart device notifications are no longer just\\nsituational alerts, they become an integral proactive component of mobile services en-\\ngaging with their users [21,22] or enabling them to manage their activity-related choices,\\nfor example, through the check-in mechanisms used in social networks [23]. Such proac-\\ntive services help to overcome the limitations of human decision-making conditioned by\\nthe naturally limited capabilities of individuals to memorize, store or organize informa-\\ntion [24,25]. Notifications give the common approach to link the current context to a sit-\\nuation which might be distant in time or/and location, while this context, in turn, can be\\n“pre-programmed” by the user as a description of a deferred action to be implemented\\nlater.\\n3. “Soft Planning”: Revisiting the Concept\\n3.1. Situational Planning by Examples\\nHere are some model scenarios illustrating the concept of soft planning:\\n1. Someone realizes that some product or household item is missing at home (Fig-\\nure 1). It is not so urgent that one must go to find it immediately. Instead, it might\\nbe helpful to be reminded while the person in charge is in or nearby the particular\\nstores, where this item can be found. Thus, exact time or even location might be\\nunknown.\\nFigure 1. Soft planning in shopping.\\n2. A traveler discovers an interesting sightseeing spot which is worth visiting at a\\nlater date (Figure 2). Thus, the exact time frame for a visit might be unknown,\\nbut a reminder could be helpful to describe a kind of deferred activity that can be\\nsuggested as soon as it happens that the traveler is nearby this location again (for\\nexample, driving in the area because of some business) but in a better season.\\nFigure 2. Soft planning while travelling.\\n3. A theatre or cinema lover would like to watch a particular performance (maybe\\nstill unreleased) as soon as it becomes available for watching or booking.\\n4. A householder needs to do some housework, where specific timing is unknown or\\nnot so important. The reminders could be helpful but only while being at home.\\n3.2. Conceptualization\\nIt is commonly agreed that conceptualization is an important process, which often\\ninvolves ontology-based approaches for system design and requirement engineering\\nstages [26,27,28]. Constructing an ontology model helps in eliminating redundant infor-\\nmation and defining project scope better. As a conceptual core of the whole project, the\\ncommon-sense ontology is linked to domain-specific ontologies and provides grounds\\nFigure 3. Iterations in model conceptualization.\\nfor requirement elicitation. Software prototyping helps us to refine the requirements and\\nrevisit the ontology models as well, and improve our understanding of the system scope\\nand boundaries (Figure 3).\\nFigure 4. Soft planning system: common-sense ontology (revised against [18]).\\nFigure 4 illustrates the conceptual model of soft planning through constructing a\\ncommon-sense ontology. The user’s activity, situation, and ideas influence the plan,\\nwhere the user is involved in the activity, and the activity is contextualized by the situa-\\ntion and suggested by the system. The user defines the desired reminders through condi-\\ntions or properties. The common-sense ontology can be naturally connected to the sub-\\nject domain ontology (such as one in Figure 5) providing grounds to discover major\\nentities to be modeled in the system and their key relationships.\\nThe condition of the reminder in this system is mainly determined by the time and\\nlocation. The location consists of a current location and a programmed location, the\\nlatter being the location generated by the system. According to the suggested model, the\\nreminders set by the user are assumed to belong to the following major classes:\\nFigure 5. Domain-specific entities.\\n1. Active reminders are to be configured by the user and processed by the system so\\nthat when the appropriate user conditions (such as being near the desired loca-\\ntion) are fulfilled, the system issues a notification.\\n2. Templates are reminder configurations stored in the system for future use to fa-\\ncilitate the future notification setup. The templates are assumed to be configured\\npartially, or suggested by the system for common or frequent scenarios.\\n3. Suspended reminders are those delayed by the user. For suspended reminders, the\\nuser may need specific configuration options referring to the earlier established\\npolicies (such as reactivate the reminder after a certain period of time, reactivate\\nit manually, or even notify the user about the existing suspended actions).\\n4. Archived reminders may be useful for the cases if the user wishes to save the\\npreviously configured reminders for future reactivation instead of trashing them.\\n3.3. Major Use Cases\\nFigure 6 shows the major scenarios to be supported by this system.\\n3.4. Reminders and Notifications: Workflow\\nBased on reminder status definitions, we defined two major stages of reminder setting\\nand notification management presented in Figures 7– 8 using the statechart behavioral\\nmodels based on Harel’s statechart formalism [29].\\nFigure 6. Major use cases in the soft planning system.\\nFigure 7 drafts the superstate Setting and configuring a reminder describing major\\nevents and actions to create a reminder, manage the lists of active and archived reminders,\\nsave, or delete reminders.\\nFigure 8 sketches the superstate Working with issued notifications supporting such\\ntransactions as “forgetting” the reminder, archiving it, keeping it active, or re-configuring\\nit (thus, triggering the system back to the superstate Setting and configuring a reminder).\\n4. Piloting Prototypes: Lessons Learned\\nTo improve our understanding of the project goals and challenges to resolve, we experi-\\nmented with several prototype implementations developed as Android applications. Ex-\\nperimental components partially address the use cases presented in Figure 6. This section\\ndescribes the lessons learned as a result of experiments with developed prototypes.\\n4.1. Geofencing in the Context of Soft Planning\\nThe trickiest problem of situational planning scenario implementation is to assure that\\nrelevant notifications are issued in connection to specific locations. To achieve this,\\nlocation-based services such as geofences are required. Geofences define virtual geo-\\ngraphic boundaries (usually connected to some point of interest) and enable registered\\napplications to trigger responses when a mobile device enters or leaves a particular area\\n(see, for example, [30]). Geofences can be set with geographic coordinates, geometric\\nFigure 7. Setting a reminder.\\nFigure 8. Working with issued notifications.\\nparameters (such as radius for circular geofences), duration, and transition types. They\\nare often connected with the points of vision while constructing traveling itineraries\\nand tourist guides [31]. Figure 9 illustrates the main scenario and the main eventual\\nissue: using the geometry-based distancing to decide on notification upon entering the\\ngeofence area might lead to annoying and unreliable notifications or never-ending re-\\nminders. Therefore, it is important to elicit a number of aspects to be taken into the\\nconsideration while implementing a practical location-based notification service:\\n1. Recommendations and proximity evaluation may differ significantly depending\\non the means of transportation.\\n2. Landscape characteristics (especially while walking or cycling may affect user’s\\nwish to proceed with the suggested deferred activity.\\nFigure 9. Location-based scenario: planning → geofencing → action.\\n3. Possible waiting time or preliminary booking required at the recommended loca-\\ntion could make a suggestion hard to follow.\\n4. Using programmed reminders together with a reasoning engine may be helpful\\nfor getting some more information about “softly” planned activities referring to a\\nselection of possible locations rather than to a specific one (like “any food store”\\nbut not a particular one).\\n5. Conversion of a reminded activity to a regular plan to be included in the calendar\\nor time scheduler might need integration with popular time management services.\\nWe used a combination of system alarms, calendars, and notifications for point-in-\\ntime and time-period reminders; and added voice input and cloud sync to make it easier\\nto use. For location-based reminders, we used a combination of Google’s geofence and\\ncloud services to synchronize the geofence coordinates created in the database with the\\ncloud. The system can send the corresponding notification when the user enters or leave\\nthe registered geofence. It is helpful for further debugging and evaluation.\\n4.2. Third-Party Services\\nThe following third-party services are shown to be helpful for developing a prototype\\nimplementation:\\n1. iFLYTEK Speech Recognition – The component is used to recognize short audio\\nand convert into text in real time [32], thus, supporting voice input.\\n2. Google Geofence – Geofencing combines awareness of the user’s current location\\nwith awareness of the user’s proximity to locations that may be of interest[33]. It\\nis used for creating location-based notifications.\\n3. Nutstore WebDav – Using WebDav and Firebase[34] helps in backing-up the\\nconfigured reminder information to ensure that users can change or use it multi-\\nple devices supporting such options as direct online editing, version control, and\\ncollaborative editing[35]. The data synchronize component uses a cloud service\\nthat supports WebDav protocol. The advantage of such an architecture is that the\\ndeveloper does not need to develop a separate server-side program. However, the\\nuser needs to be registered with the cloud service provider.\\n4.3. Client Environment\\nThe proof-of-concept client applications running under Android 10.0 run-time environ-\\nment were developed using Java and Kotlin.\\n5. Conclusion and Future Work\\nThe scenarios of soft planning may be considered as an element of activity-centric com-\\nputing paradigm addressing the multi-faceted nature of human activity in the real world,\\nwhen people tend to create “activity representations that are simple, yet flexible enough\\nto accommodate different levels of rigidity” [36] and “re-program” their future experi-\\nence based on the feedback they may have from the current activity, or from the experi-\\nence of other users.\\nThere is a number of issues requiring further efforts to move the project towards its\\npractical use:\\n• Identification of possible useful scenarios of user collaboration in setting and using\\n“soft” plans;\\n• Integration of soft planners with regular time schedulers and activity organizers;\\n• Possibilities to apply knowledge-based recommendation and NLP algorithms so\\nthat to identify and suggest the suitable scenario to the user semi-automatically.\\nThough the current stage of the project does not involve sufficient arguable experi-\\nmental results to contrast the approach against the existing human-centric solutions (usu-\\nally actively involving machine learning algorithms), we believe that the presented con-\\nceptualization creates grounds for improving personalized services and environments for\\npractical ubiquitous time and activity management.\\n\\n\\n\",\n"," 'Download\\n\\nSource\\n\\nPDF\\nActions\\n   Copy Project\\n   Word Count\\nSync\\n   Dropbox\\n   Git\\n   GitHub\\nSettings\\nCompiler\\n\\nTrack changes is on\\nTowards Demystifying Transformations of Tchaikovsky’s Children’s Album with\\nSupport of Computational Models: Problem Conceptualization\\n\\n\\nAbstract—Though the studies of rich metaphors hidden in the\\nmusical compositions lay mostly in scope of art and musicology,\\nthere is still large space for formal methods based on mathe-\\nmatical models and computer technology that can be helpful in\\ndiscovering complementary insights to how the composition is\\nstructured, what are its relationships to the precursors’ works,\\nand how it affects the later works of the same or other authors.\\nOur idea is to investigate how computational models can enhance\\nmusicology research on music style identification and comparative\\nanalysis using the case study of Tchaikovsky’s Children’s Album.\\nKeywords–Musicology; music information retrieval; human-\\ncentric computing; similarity; music modeling.\\nI. INTRODUCTION\\nIn my recent talk on Jan 6th, 2021, in the University of\\nAizu “Tchaikovsky. Children’s (?) Album: Time, Metaphors,\\nRediscoveries” [1], I discussed the phenomenon of Piotr\\nTchaikovsky’s “Children’s Album” for piano solo (Op. 39) [2].\\nThis masterpiece was composed and published as far as in\\n1878, but today it still remains one of constant topics of interest\\nfor researchers [3], [4]. Though the study of rich metaphors\\nhidden in the pieces thought to be for children rather lies in the\\nscope of art and musicology, there is still a large research space\\nfor formal methods based on mathematical and computational\\nmodels, which may give additional insights into our under-\\nstanding of the structure and organization of the whole work,\\nits relationships to precursors such as “43 Clavierst ̈ucke f ̈ur die\\nJugend” by Robert Schumann [5], as well as the reasons for\\nsignificant differences between the original manuscript and the\\nfirst published edition. Surprisingly, in musicology literature,\\none of the first careful studies of transformations between\\nthe manuscript and the published edition can be found in the\\nearly 1990s only; thus, more than 100 years after the whole\\nwork was completed [6], [7]. These studies mostly remain in\\nscope of music and art theory, with almost no involvement\\nof machine learning approaches. Today, it is commonly not\\ndisputed that computer science and artificial intelligence may\\ncontribute to musicology research on music style identification\\nand comparative analysis. In this study we try to discover\\nappropriate formal models that would enhance the analysis and\\nunderstanding of Tchaikovsky’s “Children’s Album”, which\\ncan be considered as a very good example of applied human-\\ncentric computing research in the frame of art and humanities,\\nwhere solutions cannot be designed within a certain context\\nonly, but require intensive cross-disciplinary efforts so as to\\nbridge the communities working in different contexts and using\\ndifferent vocabularies [8].\\nII. RELATED WORK\\nAccording to Nattiez, music is a symbolic fact charac-\\nterized by the complex configuration of interpretants [9]. In\\nmusic, we use various connected but independent models\\nincluding letter-based notations such as Helmholz or scientific\\npitch notation (that can be considered as simple syntax based\\nlanguage constructions), complex symbolic notations in the\\nform of graphic note scores ranging from hardly formalizable\\nancient models such as Znamenny chant, at one pole, or\\nrelatively strict Mensural notation, at another pole, up to\\nmodern sheet music (based on many rules but giving some\\nfreedom to support the individual styles of composers), piano\\nroll notation, tablature, MIDI representations, as well as audio\\nsignals and even spectral models such as acoustic fingerprints.\\nThe great variety of models used for music representation is\\none of reasons why music provides an interesting and complex\\nuse case for experimenting with information retrieval, object\\nrecognition and classification algorithms. Music representation\\ncomplexity can be explained by the presence of two arrays of\\nelements and relationships, where the first one corresponds\\nto the elements that can be treated mathematically (such as\\npitch, rhythm, or harmony), while the second one includes\\nnon-mathematical elements such as tension, expectancy, and\\nemotion [10].\\nA. Bridging the gap between pure musicology and applied\\nhuman-centric computer technology\\nCurrent approaches to music similarity evaluation (in-\\ncluding our own work on melody extraction and similarity\\nestimation using Earth Mover’s Distance algorithms [11])\\nmostly target the searching and retrieval systems including\\nwell-known apps such as Shazam [12], without a perfect fit\\nto the problems of stylistic similarity evaluation. From this\\npoint of view, models of functional representation of music\\nharmony and harmonic similarity estimation [13] seem to be\\nmore adequate to the problem of style identification. Indeed,\\nusually, listeners can recognize similarity of compositions\\nbecause of their harmonic similarity (see Figure 1). However,\\nit does not immediately lead us to clearly conclude about the\\ncomposition’s stylistic resemblances or dependencies. Even\\nharmonic equivalence may not be enough to recognize the\\nmelody, as demonstrated in [14] and later analyzed in [15]\\nin the experiments with melodies distorted by substituting the\\nnote octave by randomly selected ones within three octaves:\\nevery note in the sequence keeps its position on the scale,\\nbut the tune varies over a three-octave range (similarly to an\\nexample of such distortion shown in Figure 2).\\nFigure 1. Harmony resemblance between Beethoven’s Moonlight Sonata and\\nits variation\\nFigure 2. Distorted note sequence of Beethoven’s Moonlight Sonata with\\nkeeping harmonic equivalence)\\nHarmonic functions were core elements of SPEAC music\\nrepresentation system developed by Cope [16], [17], which is\\nan implementation of augmented transition network, a finite-\\nstate automaton with recursive succession rules between music\\nsub-phrases allowing for logical syntax substitutions [18].\\nCope’s SPEAC system is based on a hierarchical representation\\nof the structure of music composition in nested contexts\\nbeginning from notes and chords up to chapters and parts (see\\nFigure 3 (a)). Five identifiers contributing to SPEAC acronym\\nstand for statement S, preparation P , extension E, antecedent\\nA, and consequent C, all of which are kinds of abstractions\\nassigned to groups of notes “depending on levels of tension\\nbetween intervals, metrical placement, and agogic emphasis,\\nmeasured both in the preceding and following groups” [18].\\nSuccession rules defined by Cope limit possible transitions\\nbetween the SPEAC states (see Figure 3 (b)). Therefore,\\nSPEAC progressions are like genome sequences using SPEAC\\nidentifiers as bases enforced by harmonic tension weights\\nand hierarchical relationships between progressions at different\\nlevels. Modeling music structure using SPEAC-analysis can be\\na promising approach to model music style similarity through\\nSPEAC progression similarity as well as similarity between\\nthe corresponding graphs that can be constructed, specifically\\nwith respect to recent implementations available as libraries in\\nuniversal languages such as Python [19].\\nDue to a large number of applications of using deep neural\\nnetworks for object recognition and classification (especially\\nfor image recognition, including such subjective trait as image\\naesthetics), machine learning approaches and recurrent neural\\nnetworks may be promising for music style identification,\\nFigure 3. Progression bases in SPEAC system by David Cope.\\nclassification and analysis. Though, in contrast to a variety of\\nworks on computer music generation, we argue that the main\\nchallenge is not to teach AI to create art objects, but to be able\\nto help us in perceiving objects created by humans [20].\\nB. Renditions and Implications of “Children’s Album”\\nSince both Tchaikovsky and Schumann belong to the ro-\\nmantic tradition rooted in part of leitmotif music by Beethoven\\nand Wagner, on the one hand, and in the new music language of\\nGlinka, Chopin and Liszt, on the other hand, certain harmony\\nand music development similarity surely exists in their works.\\nAccording to the network of influences on classical composers\\noriginally described by Smith and Georges et al. in the original\\nClassical Music Navigator [21], Schumann is one of composers\\nwho greatly influenced Tchaikovsky (along with Balakirev,\\nBeethoven, Chopin, Delibes and others) as shown in Figure 4,\\nHowever, admitting Schumann’s influence to Tchaikovsky\\ndoes not lead us to automatically judge the “Children’s Album”\\nas an imitation of Schumann’s pieces for the young (also with\\nlong history of editions but rather few scholarly studies [23])\\neven though Tchaikovsky claimed it explicitly in the subtitle\\nfor the published edition “24 simple pieces for children like\\nSchumann” (but not in the manuscript! [24]. What if this\\nsubtle (?) subtitle is a kind of hint that Tchaikovsky gave\\nus? Like saying: “Well, it is definitely not “like Schumann”!\\nShould you then believe in the appropriateness of all made\\ntransformations?” These transformations (see Figure 5) destroy\\nthe structure of the album as an indissociable whole, and\\ndeform the micro-cycles existing in the manuscripts (where\\nthe Doll cycle is the clearest case), as well as harmonic links\\nsuch as between the first and the last pieces in the manuscript,\\n“Morning prayer” and “The hurdy-gurdy man is singing”,\\nrespectively.\\nAn idea that changes in the order of compositions between\\nthe manuscript and first published edition were mistakenly in-\\ntroduced by the publisher could not be accepted as convincing\\nenough: indeed, Tchaikovsky approved this version. Nekhaeva\\nsuggested that these transformations can be considered as a\\n“gesture of the composer, a natural desire to overcome the\\ntemporary barrier and directly appeal to future generations of\\nmusicians” [4]. This opinion supports an existing hypothesis\\nclaiming that Tchaikovsky probably preferred to hide some\\nmetaphors so that they are not so explicitly exposed as in the\\nmanuscript. From the perspective of musicology, we could not\\nexpect to find a final answer (and perhaps it is not needed).\\nInstead, a possibility to incorporate formal computational\\napproaches into informal art discourse can, produce a number\\nof important additional insights for better understanding of\\ngenesis of one of Tchaikovsky’s masterpieces in piano music.\\n',\n"," '\\n\\nWith the rapid growth of the information communication technology sector, datacenters are  experiencing an exponential increase in energy consumption. One of the possible approaches to reducing energy consumption is to increase the ambient temperature, thus lowering the energy consumed in the cooling systems. However, it puts forward more stringent requirements for the reliability and durability of the equipped optoelectronic components. Herein, we fabricate and demonstrate silicon-polymer hybrid (SPH) modulators which feature excellent reliability with an exceptional signal fidelity retained at extremely-high ambient temperatures up to 110 °C and even after a long-period exposure to high temperatures, and meanwhile support ultra-fast single-lane data rates up to 200 gigabits per second. This is achieved by taking advantage of the high electro-optic (EO) activities (in-device n3r33=1021 pm V-1), low dielectric constant, low propagation loss (α, 0.22 dB/mm) and ultra-high glass transition temperature (Tg, 172 °C) of the developed side-chain EO polymers. The presented SPH modulator simultaneously fulfils the requirements on the bandwidth, EO efficiency, and thermal stability for EO modulators. It could provide ultra-fast and reliable interconnects for energy-hungry and harsh-environment applications such as data communication, 5G, autonomous driving, LiDAR, and aviation spacecraft systems, effectively addressing the energy consumption issue for the next-generation optical communication networks. \\n\\nMain\\nRecently, the traffic in data communication has seen explosive growth due to the emergence of bandwidth-hungry applications such as high-definition streaming media, 5G, cloud-based service delivery, and Internet-of-Things. The rapid growth in datacentre’s count and power density is leading to a dramatic increase in demand for energy. It is predicted that by 2025, datacentres will be accountable for 20% of worldwide electricity consumption [1, 2]. The cooling system alone may account for up to 40% in average of the energy demands of a datacentre [3]. Therefore, to increase energy efficiency in datacentres, one effective solution is to raise the operating temperature, thus lowering the energy consumption in the cooling systems. 4% operating costs could be saved from cooling for every 1 °C increase in operating temperature [4]. However, this puts forward higher requirements for the reliability and long-term stability of the components equipped in the datacentres, especially transceivers, at high ambient temperatures. As one of the key components in the transceiver, electro-optic (EO) modulators have been realized on several photonic platforms, such as silicon [5-8], indium phosphide [9-11], organic [12-16], lithium niobate [17-22], and plasmonics [23-29], each with their own advantages, such as enabling the integration with complementary metal-oxide-semiconductor (CMOS) electronics, low drive voltages, ultra-high bandwidths up to the terahertz regime [22-24], and small footprints. Considering the ever-increasing energy consumption and emerging applications in harsh environments, apart from the aforementioned aspects, the reliability of the EO modulator at high ambient temperatures is another important aspect to be investigated. Recently, several directly-modulated quantum-dot lasers [30] and distributed feedback (DFB) [31-32] have been successfully demonstrated by operating at an elevated temperature up to 80 °C with data modulations at 25 Gbaud and 53 Gbaud, respectively. On the other hand, external EO modulators incorporating organic EO materials such as silicon-polymer hybrid (SPH),silicon-organic hybrid (SOH), and plasmonic-organic hybrid (POH) modulators have shown outstanding thermal reliability owing to the intrinsic high glass transition temperatures (Tg) of the deployed organic EO materials [25, 29, 33-36]. We have recently reported a thermally-stable EO polymer modulator demonstrating stable performance of its static parameter (V\\uf070) at elevated ambient temperatures up to 105 °C for 2,000 hours [33]. Thereafter, we have further optimized the shape, the nonlinearity, and the Tg of the EO polymer using advanced molecular engineering, and improved the electronic and photonic circuits, especially the electrode design. Herein, we demonstrate a high-temperature-resistant ultra-high-speed SPH modulator, showing an extremely-high bearable ambient temperature up to 110 °C with maintained high-speed signal fidelity, meanwhile ensuring high EO activities (in-device EO figure of merit: n3r33=1021 pm V-1 at 1.55 \\uf06dm, where n is the refractive index and r33 the in-device EO coefficient), and achieving up to 200 Gbit s-1 aggregate data rates, specifically, up to 120 Gbit s-1 with on-off keying (OOK) and up to 200 Gbit s-1 with 4-level pulse-amplitude modulation (PAM4). Furthermore, the excellent signal fidelity at 100 Gbit s-1 is also experimentally confirmed even after a long-period (100 hours) exposure to 90 °C, thus validating the long-term thermal stability of the modulator. No energy-consuming temperature stabilization unit is deployed in the system. The simultaneous fulfilments of high thermal stability and ultra-high speed signalling make technological inroads into energy-efficient and high-throughput datacentres. In addition, it can also be used to implement reliable and ultra-fast interconnects in harsh-environment applications. \\n\\nResults\\nSilicon-polymer hybrid (SPH) modulator. To fabricate the SPH modulator, we prepare side-chain EO polymers with an ultra-high Tg of up to 172 °C, which are synthesized according to a modified procedure based on [37]. As shown in Fig. 1a, a chromophore with high-molecular hyper-polarizability is attached to the polymer backbone with a high loading density to realize the second-order nonlinear effect, i.e. Pockels effect, and provide a high EO coefficient, thus enabling efficient EO modulations. Meanwhile, the bulky adamantyl unit branched in the acrylate polymer leads to a high Tg. Since the thermal relaxation of acentric chromophore order in the poled film is negligible as long as the storage temperature is well below the Tg, the EO polymers with high Tg could offer good thermal stability. In the polymer synthesis, the loading densities of the chromophore and adamantyl can be simply adjusted, resulting in EO polymers with different Tg and EO efficiencies. A Tg of EO polymers up to 172 °C is obtained with the loading density of adamantyl at 34 wt%, while the loading density of chromophore of around 37 wt% leads to an effective in-device EO figure of merit (n3r33) of around 1021 pm V-1 at a wavelength of 1.55 \\uf06dm (Supplementary Note 2). Moreover, to further enhance the thermal stability of the EO polymer, dialysis purification is performed to remove polymers with low molecular weights. In contrast, the conventional EO polymers are prepared by mixing EO chromophores in the polymer hosts. However, such guest-host polymers could not sufficiently meet the demands for industrial applications due to incomplete thermal reversibility with molecular disordering at elevated temperatures. \\nThe synthesized EO polymer is then used to fabricate the SPH modulator, which is fabricated in a travelling-wave Mach-Zehnder interferometer (MZI) configuration, consisting of ultra-thin silicon core, side-chain polymer, sol-gel SiO2 claddings and electrodes. Figure 1b illustrates the schematic structure of the fabricated SPH layer by layer. The MZI waveguide is patterned on a 40 nm-thick silicon layer. The strip waveguide consists of two Y-junctions which split and combine the optical power, and two 8 mm-long arms with a 200 μm spacing. As illustrated in the schematic of the cross section of the hybrid waveguide (Fig. 1c), a 1 µm-thick EO polymer layer is structured to be sandwiched between two cladding layers prepared from the sol-gel SiO2 resin. A top view photograph of the fabricated SPH modulator is presented in Fig. 1d with a close-up micrograph of the electrodes of the device. On the top of the device, microwave strip-line gold (Au) electrodes with a length of 8 mm are deposited on the MZI arms. To achieve a large EO bandwidth, the electrodes are operated in a travelling wave manner and optimized for impedance matching (Supplementary Note 3). Here, the thickness and width of the electrodes are designed to be 3 μm and 16 μm, respectively. More importantly, in contrast to crystalline and amorphous material modulators, the intrinsic low dielectric constant of the EO polymer results in comparable refractive indices of the microwave and light signals, thus leading to negligible velocity mismatching between them to enable high-speed and broadband operations of the device. According to the measured frequency response of the fabricated SPH modulator shown in Fig. 1f, although the analyser has a limited frequency bandwidth (see Method), a 3-dB bandwidth of 68 GHz is observed while the 6-dB bandwidth of the device is presumed to be over 70 GHz. \\nTo ensure a high EO efficiency, in addition to the optimization of the material’s EO coefficient, the light should be well confined between silicon and polymer layers. Figure 1e shows the calculated field distribution of the TM0 mode. Given that the refractive indices of the polymer and silicon are 1.67 and 3.48, respectively, according to the modal calculation (Supplementary Note 1), the confinement factor (\\uf047) defined by the ratio of the optical power in the EO polymer to the total power is 73.8% with a 4 µm-wide and 40 nm-thick ultra-thin silicon core, which is higher than conventional silicon on insulator (SOI) strip waveguides with typical geometrical dimensions , e.g. \\uf047≈15 % with a 220\\uf0b4500 nm silicon core [38]. With such a large confinement factor in the modulator, the induced refractive-index change in the EO polymer cladding, which is proportional to the applied electrical field, results in a corresponding change of mode field distribution, and consequently accomplishes effective modulations of the guided light. The advantage of the intrinsic high EO coefficient of the polymer, together with the excellent confinement of light in the waveguide, enables a high EO efficiency, leading to a measured \\uf070-voltage-length product (V\\uf070∙L) of 1.44 V∙cm at a wavelength of 1.55 \\uf06dm. In addition, in such a shallow silicon strip [39-41], the fundamental optical mode near the sidewall occupies a small fraction of the total available optical power, thus leading to a propagation loss as low as 0.22 dB/mm (Supplementary Note 5). \\n\\nUp to 200 Gbit s-1 signalling. As mentioned above, the optimizations of the electronic circuit such as electrode dimensions, impedance matching, minimization of frequency-dependent RF propagation loss and coupling loss, together with the intrinsic low dielectric constant of EO polymer, lead to a measured 3 dB bandwidth of around 68 GHz, showing the possibility for beyond 100 Gbit s-1 data operation. The bandwidth expansion is expected by further optimizing the travelling-wave electrode design and fabrication. Using binary driving electronics, OOK signals at bit rates up to 120 Gbit s-1 are generated. The measured optical eye diagrams of 100 Gbit s-1 and 120 Gbit s-1 OOKs are shown in Fig. 2c and Fig. 2d, respectively, with clear eye openings. The corresponding bit-error rates (BERs) at operating rates of 90, 100 and 110 Gbit s-1 are presented in Fig. 2e when the received optical power (ROP) before the photo-detector is adjusted. No error floor is observed even for 110 Gbit s-1 OOK till BER of 10-5, which is well below the pre-KP4-FEC BER threshold (2.2\\uf0b410-4). As presumed in the bandwidth analysis, error-free operations are achieved at bit rates beyond 100 Gbit s-1, validating the high-speed operating capacity of the device.\\nMultilevel modulation is another effective method to increase the operating rate of the modulator. As shown in Fig. 3a, for PAM4 signalling, a four-level electrical signal from an arbitrary waveform generator (AWG), used as a digital-to-analog converter (DAC), at a sampling rate of 92 GSa s-1 is deployed to drive the SPH modulator. Optical PAM4 signals at symbol rates varying from 44 Gbaud to 56 Gbaud are generated, achieving operating rates up to 112 Gbit s-1. Clear eye diagrams of the obtained 104 Gbit s-1 and 112 Gbit s-1 PAM4 signals are shown in Fig. 3b and 3c, respectively. The measured BER curves of synthesized 88, 104 and 112 Gbit s-1 as functions of ROPs are depicted in Fig. 3d. No error floors are observed up to BER of 1\\uf0b410-5 for all the measured BER curves at bit rates up to 112 Gbit s-1. Instead of conventional non-return-to-zero (NRZ) pulse shape, Nyquist pulse shaping with a small roll-off factor (0.01) could be helpful to further boost the signalling speed of the device with enhanced spectral efficiency. By applying raised cosine filters over the driving electrical signals, through another DAC operated at 120 GSa s-1, Nyquist PAM4 signalling at 80, 92 and 100 Gbaud are realized by the SPH, achieving operating rates of 160, 184 and 200 Gbit s-1, respectively. The obtained eye diagrams at 184 Gbit s-1 and 200 Gbit s-1 are shown in Fig. 3e and 3f, respectively. According to the measured BER curves illustrated in Fig. 3g, the error rates below both the pre-HD-FEC (3.8\\uf0b410-3) and pre-KP4-FEC BER thresholds are well achieved for both 160 Gbit s-1 and 184 Gbit s-1 Nyquist PAM4. On the other hand, for the synthesized 200 Gbit s-1 Nyquist PAM4, the error rates below the pre-HD-FEC threshold are also observed. The successful syntheses of beyond 100 Gbit s-1 (up to 200 Gbit s-1) signals in either OOK or PAM4 verify the ultra-fast operating capacity of the fabricated SPH modulator. \\n\\nHigh-speed operation at elevated temperature. In addition to ultra-fast signalling, the deployment of high-Tg EO polymer in the SPH modulator allows for high-temperature operation without failure, which is essential for improving the datacentres’ energy efficiency by raising the ambient temperature, and crucial for providing components suitable for harsh environments in 5G [42], LiDAR, satellites, and avionics applications. We have tested the long-term thermal reliability and stability of the device in accordance with Telcordia standards of high-temperature storage. It has been confirmed that the static electro-optic characteristic (V\\uf070) of our EO polymer modulator converges to a constant long-term stable value over a long duration (2,000 hours) at elevated temperatures of 85 °C and 105 °C [33].\\nIn order to further comprehensively assess the high-temperature reliability of SPH modulators, instead of monitoring static parameters of the device such as V\\uf070, optical insertion loss, and S parameters, it would be more effective to measure dynamic properties like BER or Q factor of the signals synthesized from the device during or after the exposure to elevated ambient temperatures. In contrast to previous investigations, for the first time to our knowledge, the thermal reliability of the fabricated SPH modulator is assessed by investigating its high-speed performance when the operating ambient temperature is adjusted over an extraordinary wide range (25 °C ~110 °C). For the test, our modulator is first configured as a binary OOK transmitter by feeding binary driving electrical signals at bit rates of 56, 100 and 110 Gbit s-1. The corresponding Q factors of the synthesized OOK signals are measured and shown in Fig. 4a when the ambient temperature is adjusted over the aforementioned range. As shown in the inset of Fig. 4a, clear eye openings are observed at 56 Gbit s-1 and 100 Gbit s-1 even at extremely-high temperatures up to 105 °C. When the ambient temperature is increased from 25 °C to 110 °C, 56 Gbit s-1 OOK shows less than 0.5 dB decrease in the measured Q factor. The measured Q factors of 100 Gbit s-1 and 110 Gbit s-1 OOK signals decrease slightly (~1 dB) when the ambient temperature is increased till 100 °C. Although higher bit-rate signalling becomes more sensitive to ambient temperatures, the change of Q factor within the wide temperature range is less than 2.5 dB for both 100 Gbit s-1 and 110 Gbit s-1 signals. The measured Q factors over the wide temperature range are all above 3.51, corresponding to the KP4 pre-FEC BER threshold. No abrupt Q factor degradations are observed even for OOKs operating at 100 Gbit s-1 and beyond. As presented in the next section, a negligible penalty is observed once the device is cooled down to room temperature even after a long-period high-temperature (90 °C) exposure, which is in line with previous investigations [33]. In addition, the thermal reliability of the SPH modulator is also investigated when the device is operating at 200 G bit s-1 with PAM4. The BERs of the synthesized 200 G bit s-1 PAM4 are measured over the aforementioned ambient temperature range and plotted in Fig. 4b, only showing a slight increase of less than one order of magnitude. These results indicate the excellent thermal reliability of the SPH modulator at the extremely high temperature, which is attributed to the stable ordering of the chromophore in the synthesized ultra-high-Tg EO polymer. More importantly, the high EO activities of the SPH modulator are well retained over an extraordinary wide temperature range, which enables up to 200 Gbit s-1 high-speed operating even at extremely-high ambient temperatures up to 110 °C. \\n\\nHigh-speed operation after long-period high-temperature storage. As for the organic electro-optic material integrated in SPH devices, de-poling (molecular disordering) usually occurs around less than 30 °C below the glass transition temperature [43, 44], implying that the upper temperature limit for long-term stable operation of the side-chain polymer used in our SPH device (Tg: 172 °C) is ~140 °C. To validate the high-speed functionality of the SPH modulator even after a burn-in high-temperature test, the BER performance of the generated 100 Gbit s-1 signals is evaluated after a 100-hour exposure to a high temperature (90 °C). For comparison, the measured curve of BER versus the ROP before the burn-in test is measured and plotted as well. As shown in Fig. 5a, error-free operations are confirmed for 100 Gbit s-1 signals before and after the burn-in test with BER up to 1\\uf0b410-5, which is below the KP4 pre-FEC BER threshold. With respect to the BER curve before the burn-in test, negligible power penalty is observed even after the 100-hour exposure to high temperature, which is consistent with the observed negligible degradation of V\\uf070 after a period of high-temperature storage [33]. Clear eye openings are observed in the measured eye diagrams in both cases, shown in Fig. 5b and 5c, respectively. The results indicate that the high EO activities could be well maintained even after being stored at elevated temperatures for a long period, demonstrating an excellent long-term thermal stability of the fabricated broadband SPH modulator. \\n\\nThermal reliability comparison. Electro-optic modulators are mainly formed on structures like micro-ring resonator (MRR), Mach-Zehnder interferometer (MZI), or a combination of MRR and MZI. MRR exhibits a compact footprint but shows a strong sensitivity to ambient temperature fluctuations especially on a high thermo-optic (TO) coefficient substrate such as silicon. Most of the proposed approaches to overcome the thermal instability issue in MRR either require significant power consumption in the temperature stabilization unit [45, 46] or incorporate negative TO coefficient materials at the cost of increased complexity [47-49]. Through the active compensation approach, athermal silicon MRR modulators with a good thermal tolerance against 15 K [45] or 7.5 °C [46] thermal drift have been successfully demonstrated. In contrast, MZI and MRR-assisted MZI [50] structures possess better thermal stability. These structures have been demonstrated on various photonic platforms such as SOI, lithium niobate, indium phosphide (InP), SOH, plasmonic-organic hybrid (POH), and SPH. Table 1 lists and compares some modulator metrics of our device with prior demonstrations, focusing on the operating speed and the operating temperature range when the devices are high-speed activated. Note that, for the POH [25, 29] and InP [10] modulators, the high-temperature reliability is evaluated by monitoring the static properties of the devices at a single frequency such as V\\uf070, modulation index or efficiency, rather than evaluating the high-speed signal quality from the devices. To compare the EO activity of these devices across different photonic platforms, herein a voltage-bandwidth parameter is defined by the ratio of the measured 3 dB bandwidth over the half-wave voltage of the device, which is also known as bandwidth half-wave voltage ratio (BVR). A larger BVR indicates a high-speed operating ability with a low driving voltage, corresponding to higher EO activity and efficiency. As shown in Table 1, our device shows the highest EO activity (BVR: 38 GHz V-1). As another important figure of merit for electro-optic modulators, loss-efficiency product is quantified by the multiplication of π-voltage-length product (V\\uf070∙L) and the propagation loss (α), i.e. α∙V\\uf070∙L. In comparison to other devices in Table 1, although the SPH modulator presented in this work has no clear advantage in terms of the footprint, it features the best loss-efficiency product (3.6 V∙dB), indicating an excellent trade-off between optical loss and modulation efficiency. More importantly, it is the only one achieving up to 200 Gbit s-1 operations under an extraordinary wide temperature range (25 °C ~110 °C). The fabricated EO polymer modulator on a silicon substrate represents a new paradigm in EO modulator development for ensuring excellent high-temperature reliability and up to 200 Gbit s-1 ultra-high-speed operations. It simultaneously meets the requirements for both the operating speed and thermal reliability of optoelectronic components in future energy-efficient datacentres and harsh-environment applications. \\n\\n\\n\\nDiscussion. The power dissipation of the modulator driver accounts for the great portion of the overall energy consumption of the transceivers in the datacentres. In the experiments, with the device length of 8 mm, up to 200 Gbit s-1 signalling is performed using the fabricated SPH modulator with a drive voltage of around 1.3 Vpp which is compatible with CMOS voltage levels. The corresponding electrical energy dissipation of the modulator is 42 fJ bit-1 (see Methods). Further optimization of the loss of the microwave strip lines and the improvement of the poling efficiency could further improve the EO efficiency of the modulator, thus reducing V\\uf070, lowering the device energy consumption and facilitating the integration with CMOS circuits. \\nTo improve the energy efficiency of the datacentres, in addition to decreasing the energy consumption in the transceivers, another effective approach is to raise the ambient temperature of the datacenters, thus reducing the energy consumed in the cooling systems. In this work, our fabricated SPH modulator allows for beyond 100 Gbit s-1 high-speed signalling with excellent signal fidelity at an extremely-high ambient temperature of 110 °C, and shows negligible power penalty even after undergoing thermal exposure up to 90 °C for 100 hours, owing to the fast response, high EO activity, and ultra-high thermal reliability (Tg=172 °C) of the deployed EO side-chain polymer. The reliable and durable SPH modulator allows for raising the ambient operating temperature in datacentres, thus saving the running cost and improving energy efficiency. It could also provide EO components suitable for applications in harsh environments. For example, 5G devices are usually installed in aggressive environments (uncontrolled locations) such as on streetlamps, traffic lights, rooftops, stadiums, and parking garages, where the devices are subject to high temperatures and thermal shocks. Even in such aggressive environments, our thermal-reliable EO modulator could provide an effective solution to ensure high throughput and reliable network connections.\\n\\nFuture work. The successful demonstration of the high-temperature-resistant SPH modulator operating at up to 200 Gbit s-1 relies on a combination of effective waveguide structure and highly efficient nonlinear organic EO material. The presented modulator is fabricated on an ultra-thin silicon strip waveguide, featuring ease of fabrication and low propagation loss [39-40]. However, these features come at the cost of millimeter dimensions. In contrast, with device architectures based on a silicon slot waveguide [34, 35, 51, 52] or a metal-insulator-metal plasmonic slot waveguide [23-28] which are deployed in SOH and POH modulators, both optical and electrical fields could be tightly confined into nanoscopic dimensions with enhanced nonlinearities and concentrated electrical field, thereby reducing the \\uf070-voltage-length product in electro-optic modulators, and shrinking the footprint to sub-millimeter [53, 54] or micrometer [23-28] scales. The waveguide architecture of our device could be further optimized to realize a more compact footprint, which is of utmost importance in view of future ultra-dense high-speed parallelized interconnects to facilitate dense modulator arrays [55, 56] for space-division multiplexing applications, and enable advanced complex modulations on a most compact footprint for coherent communications [44, 57]. Besides, the synthesised side-chain polymer could be overlaid on the waveguide core as a cladding material to counterbalance the core’s thermo-optic coefficient, yielding athermal and high-speed MRRs with ultra-compact footprints [48, 49]. As another important aspect of the SPH device, owing to the recent advances in material synthesis and molecular design, more efficient and thermally-stable organic electro-optic materials with higher Tg (175 °C [58], 194 °C [33], and 210 °C [59]) and improved electro-optic activities (Pockels coefficient: 390 pm/V in device at 1.55 \\uf06dm [51, 52]; >290 pm/V in thin films at 1.31 \\uf06dm [58]) are emerging. By incorporating such advanced organic electro-optic materials to effective device architectures, the long-term thermal stability and electro-optic performance of SPH modulators could be substantially improved with a compact footprint in the near future. \\n\\n\\nMethods\\nDevice fabrication. The SPH modulator is fabricated on a standard 4-inch silicon wafer with a 2 μm-thick oxide layer. The bottom SiO2 cladding is firstly prepared on the bottom electrode of 1 μm-thick aluminum from the solution of a sol-gel trimethoxysilyl derivative. After baking the spin-coated film for the sol-gel reaction at 120 °C~140 °C for 3 hours, the 3 μm-thick bottom SiO2 cladding is obtained. A 40 nm-thick silicon layer is deposited by using plasma-enhanced chemical vapor deposition (PECVD, SAMCO, PD-220NL). An MZI structure is then patterned onto the silicon layer by electronic beam lithography (EBL, Elionix, ELS-G100) followed by inductively coupled plasmas (ICP) etching (SAMCO, RIE-400iP) with SF6 gas. The EO polymer is then spin-coated by using cyclopentanone as the solvent and baked at 105 °C for 24 hours under vacuum to form a 1 µm-thick slab. A 3 µm-thick sol-gel SiO2 layer is then spin-coated onto the EO polymer as a top cladding to protect the device during the poling and modulating processes. Travelling-wave microwave strip-line gold electrodes with a length of 8 mm are then deposited onto the device by vacuum deposition and electroplating techniques. To activate the second-order nonlinear effect in polymer, a poling voltage (300 V~400 V) is finally applied across the electrodes around Tg of the EO polymer to align chromophores. After cooling down the device to room temperature and removing the poling voltage, the molecular orientation is frozen, thereby enabling Pockels effects for electro-optic modulations.\\n\\nEnergy consumption of the modulator. The driver electronics of the modulators consume most of the overall energy of the transceiver. The electrical energy per bit dissipated in travelling-wave EO modulator could be calculated by Wbit=(Vpp/2)2/(B·R), where Vpp is the peak-to-peak voltage swing, B is the line rate and R is the driver impendence. For our device, the transmission line impedance is designed to be matched to the 50 Ω driver circuitry. Therefore, for generating 56 Gbit s-1 OOK signals, the driving voltage Vpp=1.3 V results in an electrical energy per bit of 151 fJ bit-1, while for 100 Gbit s-1 OOK systems, the corresponding electrical energy per bit is about 200 fJ bit-1 with Vpp=2.0 V. In the generation of 200 Gbit s-1 Nyquist PAM4, the energy consumption per bit is around 42 fJ bit-1 with Vpp=1.3 V. Note that with dual-drive configuration, the energy consumption could be further reduced. \\n\\nDevice bandwidth characterization. To confirm the broadband operation of the SPH modulator, a vector network analyser (Anritsu, MS4647B, bandwidth: 10 MHz~70 GHz) is used to characterize the frequency response of the fabricated SPH modulator by measuring the electro-optic S21 response. The modulated output optical signal is detected by an O/E calibration module (Anritsu, MN4765B-0072, frequency range: 70 kHz~70 GHz). The maximum measurable upper frequency of the characterization system is limited to 70 GHz. \\n\\nHigh-speed operation characterization. In the experiment for generating high-speed signals at 100 Gbit s-1 and beyond, the setups are depicted in Fig. 2a, Fig. 2b, and Fig. 3a. To generate OOK signals, high-speed electrical signals generated from a pattern generator (SHF, 12104A) followed by an electronic 2:1 multiplexer (SHF, 603A) are fed into the travelling-wave electrode on the device using an RF probe (Fig. 1d, FormFactor, ACP-65-GSG-150, frequency: DC~65 GHz, operating temperature: -65 °C ~200 °C). Another RF probe is attached to the end of the electrode with a 50 \\uf057 termination for power absorption. In the PAM4 signalling, a four-level driving electrical signal is first obtained from an AWG (Keysight, M8196A, analog bandwidth: ~32 GHz) at a 92 GSa s-1 sampling rate, and fed to the SPH modulator after power amplification via a linear driver (SHF, S804B, bandwidth: 90 kHz~60 GHz). In the synthesis of Nyquist PAM4 signals, another AWG (Keysight, M8194A, analog bandwidth: ~45 GHz) at a 120 GSa s-1 sampling rate is used for generating driving electrical signals. The digital signal processing (DSP) at the transmitter side includes up-sampling, raised cosine filtering and down-sampling to form the driving electric signals. To detect the synthesized OOK and PAM4 signals, the received signal is pre-amplified by an erbium-doped fibre amplifier (EDFA) before being fed to a photo-detector (Finisar, XPDV3120R, 3-dB cut-off frequency: 75 GHz). Linear feedforward equalizers are deployed for equalizations over the digitized data in performance analysis. \\n\\nHigh-temperature stability assessment. To systemically assess the thermal stability of the fabricated modulators, high-speed performance of the SPH is experimentally investigated when the modulator is being operated at elevated ambient temperatures and after a burn-in test at a high storage temperature. In the performance investigation at high ambient temperatures, the modulator is mounted on an optical stage with a thermoelectric-cooler (TEC) element installed, to alter the operating temperature from 25 °C to 110 °C. In the measurement, the device is activated with optical and electrical connections when temperature is adjusted. On the other hand, to perform the high-temperature storage test, the device is first stored in an oven maintained at a temperature of 90 °C in air without either optical or electrical connections. After the burn-in test, the device is taken out from the oven and then optically and electrically activated to verify its high-speed functionality at room temperature. The high-speed performance of the device before the burn-in test is also conducted at room temperature, providing a benchmark for evaluating the device performance in the long-period high-temperature storage test. \\n\\n\\n\\n\\n',\n"," 'Appointment of William S. Pfaff Jr. , 41 , as promotion manager of The Times-Picayune Publishing Company was announced Saturday by John F. Tims , president of the company . \\nPfaff succeeds Martin Burke , who resigned . \\nThe new promotion manager has been employed by the company since January , 1946 , as a commercial artist in the advertising department . \\nHe is a native of New Orleans and attended Allen Elementary school , Fortier High school and Soule business college . \\nFrom June , 1942 , until December , 1945 , Pfaff served in the Army Air Corps . \\nWhile in the service he attended radio school at Scott Field in Belleville , Ill .. Before entering the service , Pfaff for five years did clerical work with a general merchandising and wholesale firm in New Orleans . \\nHe is married to the former Audrey Knecht and has a daughter , Karol , 13 . \\nThey reside at 4911 Miles Dr .. Washington -- Thousands of bleacher-type seats are being erected along Pennsylvania Avenue between the Capitol and the White House for the big inaugural parade on Jan. 20 . \\nAssuming the weather is halfway decent that day , hundreds of thousands of persons will mass along this thoroughfare as President John F. Kennedy and retiring President Dwight D. Eisenhower leave Capitol Hill following the oath-taking ceremonies and ride down this historic ceremonial route . \\nPennsylvania Avenue , named for one of the original 13 states , perhaps is not the most impressive street in the District of Columbia from a commercial standpoint . \\nBut from a historic viewpoint none can approach it . \\nMany buildings Within view of the avenue are some of the United States government \\'s tremendous buildings , plus shrines and monuments . \\nOf course , 1600 Pennsylvania , the White House , is the most famous address of the free world . \\nWithin an easy walk from Capitol Hill where Pennsylvania Avenue comes together with Constitution Avenue , begins a series of great federal buildings , some a block long and all about seven-stories high . \\nGreat chapters of history have been recorded along the avenue , now about 169 years old . \\nIn the early spring of 1913 a few hundred thousand persons turned out to watch 5000 women parade . \\nThey were the suffragettes and they wanted to vote . \\nIn the 1920 presidential election they had that right and many of them did vote for the first time . \\nSeats on square Along this avenue which saw marching soldiers from the War Between the States returning in 1865 is the National Archives building where hundreds of thousands of this country \\'s most valuable records are kept . \\nAlso the department of justice building is located where J. Edgar Hoover presides over the federal bureau of investigation . \\nStreet car tracks run down the center of Pennsylvania , powered with lines that are underground . \\nMany spectators will be occupying seats and vantage points bordering Lafayette Square , opposite the White House . \\nIn this historic square are several statues , but the one that stands out over the others is that of Gen. Andrew Jackson , hero of the Battle of New Orleans . \\nMoving past the presidential viewing stand and Lafayette Square will be at least 40 marching units . \\nAbout 16,000 military members of all branches of the armed forces will take part in the parade . \\nDivision one of the parade will be the service academies . \\nDivision two will include the representations of Massachusetts and Texas , the respective states of the President and of Vice-President L.B. Johnson . \\nThen will come nine other states in the order of their admission to the union . \\nDivision three will be headed by the Marines followed by 12 states ; ; division four will be headed by the Navy , followed by 11 states ; ; division five , by the Air Force followed by 11 states . \\nDivision six will be headed by the Coast Guard , followed by the reserve forces of all services , five states , Puerto Rico , the Virgin Islands , Guam , American Samoa , the trust territories and the Canal Zone . \\nJackson , Miss . \\n-- What does 1961 offer in political and governmental developments in Mississippi ? ? \\nEven for those who have been observing the political scene a long time , no script from the past is worth very much in gazing into the state \\'s immediate political future . \\nThis is largely because of the unpredictability of the man who operates the helm of the state government and is the elected leader of its two million inhabitants -- Gov. Ross Barnett . \\nBarnett , who came into office with no previous experience in public administration , has surrounded himself with confusion which not only keeps his foes guessing but his friends as well . \\nConsequently , it is uncertain after nearly 12 months in office just which direction the Barnett administration will take in the coming year . \\nCould be scramble Some predict the administration will settle down during 1961 and iron out the rough edges which it has had thus far . \\nThe builtin headache of the Barnett regime thus far has been the steady stream of job-seekers and others who feel they were given commitments by Barnett at some stage of his eight-year quest for the governor \\'s office . \\nThere are many who predict that should Barnett decide to call the Legislature back into special session , it will really throw his administration into a scramble . \\nCertainly nobody will predict that the next time the lawmakers come back together Barnett will be able to enjoy a re-enactment of the strange but successful \" honeymoon \" he had in the 1960 legislative session . \\nIf Barnett does n\\'t call a special session in 1961 , it will be the first year in the last decade that the Legislature has not met in regular or special session . \\nThe odds favor a special session , more than likely early in the year . \\nDistricts issue Legislators always get restless for a special session ( whether for the companionship or the $ 22.50 per diem is not certain ) and if they start agitating . \\nBarnett is not expected to be able to withstand the pressure . \\nThe issue which may make it necessary to have a session is the highly sensitive problem of cutting the state \\'s congressional districts from six to five to eliminate one congressional seat . \\nWith eyes focused on the third congressional district , the historic Delta district , and Congressman Frank E. Smith as the one most likely to go , the redistricting battle will put to a test the longstanding power which lawmakers from the Delta have held in the Legislature . \\nMississippi \\'s relations with the national Democratic party will be at a crossroads during 1961 , with the first Democratic president in eight years in the White House . \\nSplit badly during the recent presidential election into almost equally divided camps of party loyalists and independents , the Democratic party in Mississippi is currently a wreck . \\nAnd there has been no effort since the election to pull it back together . \\nFuture clouded Barnett , as the titular head of the Democratic party , apparently must make the move to reestablish relations with the national Democratic party or see a movement come from the loyalist ranks to completely bypass him as a party functionary . \\nWith a Democratic administration , party patronage would normally begin to flow to Mississippi if it had held its Democratic solidarity in the November election . \\nNow , the picture is clouded , and even US Sens. James O. Eastland and John C. Stennis , who remained loyal to the ticket , are uncertain of their status . \\nReports are that it is more than probable that the four congressmen from Mississippi who did not support the party ticket will be stripped of the usual patronage which flows to congressmen . \\nBaton Rouge , La . \\n-- The Gov. Jimmie H. Davis administration appears to face a difficult year in 1961 , with the governor \\'s theme of peace and harmony subjected to severe stresses . \\nThe year will probably start out with segregation still the most troublesome issue . \\nBut it might give way shortly to another vexing issue -- that of finances in state government . \\nThe transition from segregation to finances might already be in progress , in the form of an administration proposal to hike the state sales tax from 2 per cent to 3 per cent . \\nThe administration has said the sales tax proposal is merely part of the segregation strategy , since the revenues from the increase would be dedicated to a grant in aid program . \\nBut the tardiness of the administration in making the dedication has caused legislators to suspect the tax bill was related more directly to an over-all shortage of cash than to segregation . \\nLegislators weary Indeed , the administration \\'s curious position on the sales tax was a major factor in contributing to its defeat . \\nThe administration could not say why $ 28 million was needed for a grant-in-aid program . \\nThe effectiveness of the governor in clearing up some of the inconsistencies revolving about the sales tax bill may play a part in determining whether it can muster the required two-thirds vote . \\nThe tax bill will be up for reconsideration Wednesday in the House when the Legislature reconvenes . \\nDavis may use the tax bill as a means to effect a transition from special sessions of the Legislature to normalcy . \\nIf it fails to pass , he can throw up his hands and say the Legislature would not support him in his efforts to prevent integration . \\nHe could terminate special sessions of the Legislature . \\nActually , Davis would have to toss in the towel soon anyway . \\nMany legislators are already weary and frustrated over the so-far losing battle to block token integration . \\nThis is not the sort of thing most politicos would care to acknowledge publicly . \\nThey would like to convey the notion something is being done , even though it is something they know to be ineffectual . \\nUnderlying concern Passage of the sales tax measure would also give Davis the means to effect a transition . \\nHe could tell the Legislature they had provided the needed funds to carry on the battle . \\nThen he could tell them to go home , while the administration continued to wage the battle with the $ 28 million in extra revenues the sales tax measure would bring in over an eight months period . \\nIt is difficult to be certain how the administration views that $ 28 million , since the views of one leader may not be the same as the views of another one . \\nBut if the administration should find it does not need the $ 28 million for a grant-in-aid program , a not unlikely conclusion , it could very well seek a way to use the money for other purposes . \\nThis would be in perfect consonance with the underlying concern in the administration -- the shortage of cash . \\nIt could become an acute problem in the coming fiscal year . \\nIf the administration does not succeed in passing the sales tax bill , or any other tax bill , it could very well be faced this spring at the fiscal session of the Legislature with an interesting dilemma . \\nSince the constitution forbids introduction of a tax bill at a fiscal session , the administration will either have to cut down expenses or inflate its estimates of anticipated revenues . \\nConstant problem In either case , it could call a special session of the Legislature later in 1961 to make another stab at raising additional revenues through a tax raiser . \\nThe prospect of cutting back spending is an unpleasant one for any governor . \\nIt is one that most try to avoid , as long as they can see an alternative approach to the problem . \\nBut if all alternatives should be clearly blocked off , it can be expected the Davis administration will take steps to trim spending at the spring session of the state Legislature . \\nThis might be done to arouse those who have been squeezed out by the trims to exert pressure on the Legislature , so it would be more receptive to a tax proposal later in the year . \\nA constant problem confronting Davis on any proposals for new taxes will be the charge by his foes that he has not tried to economize . \\nAny tax bill also will revive allegations that some of his followers have been using their administration affiliations imprudently to profit themselves . \\nThe new year might see some house-cleaning , either genuine or token , depending upon developments , to give Davis an opportunity to combat some of these criticisms . \\n ']"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["documents= list(df['message'])\n","tf_vectors, tf_vectorizer = tokenizeFunc(documents)\n","documents\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#[remove]Dataframeや単語リストが一つのDFで十分な場合削除　7/25日米田\n","\n","# tf_vectors_Q, tf_vectorizer_Q = tokenizeFunc(df_Q)\n","# tf_vectors_K2, tf_vectorizer_K1 = tokenizeFunc(df_K1)\n","# tf_vectors_K2, tf_vectorizer_K2 = tokenizeFunc(df_K2)\n","# tf_vectors_ref, tf_vectorizer_ref = tokenizeFunc(df_ref)\n","\n","# words_Q=tf_vectorizer_Q.get_feature_names_out()\n","# words_K1=tf_vectorizer_K1.get_feature_names_out()\n","# words_K2=tf_vectorizer_K2.get_feature_names_out()\n","# words_K3=tf_vectorizer_ref.get_feature_names_out()"]},{"cell_type":"markdown","metadata":{},"source":["## Make a words dictionary in all documents "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":474,"status":"ok","timestamp":1689127987186,"user":{"displayName":"米田武士","userId":"14881788265874075884"},"user_tz":-540},"id":"mblTowsDdYTg","outputId":"d0e28c64-bf6a-46f4-f4b3-930bed4f76a8"},"outputs":[],"source":["# 作成された辞書を作る　:トレインデータ・テストデータ両方に対応\n","words=tf_vectorizer.get_feature_names_out()"]},{"cell_type":"markdown","metadata":{},"source":["## Make the Words frequency matrix "]},{"cell_type":"markdown","metadata":{},"source":["### This Matrix's row indices ared corresponding with a document in the "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#First row: dataset Q\n","#Second row: dataset K1\n","#Third row: dataset k2\n","#Fourth row: dataset ref"]},{"cell_type":"markdown","metadata":{},"source":["### This Matrix's col indices are corresponding with a word in the above document"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["array(['000', '0072', '01', ..., 'zone', 'µm', 'μm'], dtype=object)"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["words"]},{"cell_type":"markdown","metadata":{},"source":["## Insert Words Frequemcy Vector into 'tf' Column in DF for Each Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tf_mat = tf_vectors.toarray()\n","del tf_vectors\n","df['tf'] = tf_mat.tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>author</th>\n","      <th>message</th>\n","      <th>tf</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Q DATASET</td>\n","      <td>\\n\\nHowever, there are frequent situations whe...</td>\n","      <td>[0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>K1 DATASET</td>\n","      <td>Download\\n\\nSource\\n\\nPDF\\nActions\\n   Copy Pr...</td>\n","      <td>[0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>K2 DATASET</td>\n","      <td>\\n\\nWith the rapid growth of the information c...</td>\n","      <td>[2, 1, 1, 7, 22, 3, 2, 4, 1, 13, 4, 1, 6, 1, 0...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>BROWN DATASET</td>\n","      <td>Appointment of William S. Pfaff Jr. , 41 , as ...</td>\n","      <td>[1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 2, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          author                                            message  \\\n","0      Q DATASET  \\n\\nHowever, there are frequent situations whe...   \n","1     K1 DATASET  Download\\n\\nSource\\n\\nPDF\\nActions\\n   Copy Pr...   \n","2     K2 DATASET  \\n\\nWith the rapid growth of the information c...   \n","3  BROWN DATASET  Appointment of William S. Pfaff Jr. , 41 , as ...   \n","\n","                                                  tf  \n","0  [0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, ...  \n","1  [0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, ...  \n","2  [2, 1, 1, 7, 22, 3, 2, 4, 1, 13, 4, 1, 6, 1, 0...  \n","3  [1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 2, ...  "]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WNCL7tHudYTh"},"outputs":[],"source":["#[remove]ここの記述で必要な部分は上記に記述。そのた不要であれば削除\n","\n","#tfidf_mat = tfidf_vectors.toarray() # dead every time\n","#del tfidf_vectors\n","# tf_mat = tf_vectors.toarray()\n","# del tf_vectors\n","# df['tf'] = tf_mat.tolist()\n","#df['tfidf'] = tfidf_mat.tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZSGL6Vbkngv5"},"outputs":[],"source":["#[remove]今回はからのデータセットが存在しないため。確認後削除\n","\n","# 0 ベクトルを消去 Normalization のため\n","# for i, vec in enumerate(df['tf']):\n","#     if sum(vec) == 0:\n","#         df = df.drop(i)\n","\n","# df = df.reset_index(drop=True)"]},{"cell_type":"markdown","metadata":{},"source":["# 5. display the first 20 words of each dataset \n","\n","#Feature5"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def dispAndMakeWordFreq(df, words, author = 0):\n","    data = df.loc[author]\n","    freq = data['tf']\n","    wf = pd.DataFrame({'words': words, 'frequency': freq})\n","    wordli = []\n","    freqli =[]\n","    wordindexli = []\n","    for key, data in wf.iterrows():\n","        if(int(data[1]) != 0):\n","            wordli.append(data[0])\n","            freqli.append(data[1])\n","            wordindexli.append(key)\n","        \n","    return pd.DataFrame({'words': wordli, 'frequency': freqli, 'wordIndex': wordindexli})\n","    \n"]},{"cell_type":"markdown","metadata":{},"source":["## Words Frequency of Dataset Q"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>words</th>\n","      <th>frequency</th>\n","      <th>wordIndex</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>the</td>\n","      <td>186</td>\n","      <td>2399</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>of</td>\n","      <td>89</td>\n","      <td>1678</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>and</td>\n","      <td>74</td>\n","      <td>249</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>to</td>\n","      <td>70</td>\n","      <td>2450</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>for</td>\n","      <td>55</td>\n","      <td>1037</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>in</td>\n","      <td>54</td>\n","      <td>1251</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>be</td>\n","      <td>45</td>\n","      <td>351</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>or</td>\n","      <td>40</td>\n","      <td>1715</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>planning</td>\n","      <td>33</td>\n","      <td>1812</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>is</td>\n","      <td>31</td>\n","      <td>1330</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>are</td>\n","      <td>31</td>\n","      <td>285</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>as</td>\n","      <td>28</td>\n","      <td>298</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>time</td>\n","      <td>28</td>\n","      <td>2441</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>location</td>\n","      <td>27</td>\n","      <td>1442</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>user</td>\n","      <td>26</td>\n","      <td>2539</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>based</td>\n","      <td>20</td>\n","      <td>347</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>that</td>\n","      <td>20</td>\n","      <td>2398</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>system</td>\n","      <td>19</td>\n","      <td>2351</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>such</td>\n","      <td>18</td>\n","      <td>2320</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>figure</td>\n","      <td>18</td>\n","      <td>994</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       words  frequency  wordIndex\n","0        the        186       2399\n","1         of         89       1678\n","2        and         74        249\n","3         to         70       2450\n","4        for         55       1037\n","5         in         54       1251\n","6         be         45        351\n","7         or         40       1715\n","8   planning         33       1812\n","9         is         31       1330\n","10       are         31        285\n","11        as         28        298\n","12      time         28       2441\n","13  location         27       1442\n","14      user         26       2539\n","15     based         20        347\n","16      that         20       2398\n","17    system         19       2351\n","18      such         18       2320\n","19    figure         18        994"]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["wf_list_Q = dispAndMakeWordFreq(df,words, author = 0)\n","wf_list_Q = wf_list_Q.sort_values('frequency', ascending=False)\n","wf_list_Q = wf_list_Q.reset_index(drop=True)\n","wf_list_Q.head(20)"]},{"cell_type":"markdown","metadata":{},"source":["## Words Frequency of Dataset K1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>words</th>\n","      <th>frequency</th>\n","      <th>wordIndex</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>the</td>\n","      <td>78</td>\n","      <td>2399</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>of</td>\n","      <td>66</td>\n","      <td>1678</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>and</td>\n","      <td>50</td>\n","      <td>249</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>in</td>\n","      <td>42</td>\n","      <td>1251</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>to</td>\n","      <td>39</td>\n","      <td>2450</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>as</td>\n","      <td>32</td>\n","      <td>298</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>music</td>\n","      <td>25</td>\n","      <td>1598</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>for</td>\n","      <td>17</td>\n","      <td>1037</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>is</td>\n","      <td>17</td>\n","      <td>1330</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>be</td>\n","      <td>16</td>\n","      <td>351</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>on</td>\n","      <td>15</td>\n","      <td>1684</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>tchaikovsky</td>\n","      <td>13</td>\n","      <td>2369</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>by</td>\n","      <td>13</td>\n","      <td>409</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>that</td>\n","      <td>12</td>\n","      <td>2398</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>can</td>\n","      <td>12</td>\n","      <td>420</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>not</td>\n","      <td>11</td>\n","      <td>1649</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>between</td>\n","      <td>11</td>\n","      <td>376</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>such</td>\n","      <td>11</td>\n","      <td>2320</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>similarity</td>\n","      <td>11</td>\n","      <td>2188</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>models</td>\n","      <td>10</td>\n","      <td>1562</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          words  frequency  wordIndex\n","0           the         78       2399\n","1            of         66       1678\n","2           and         50        249\n","3            in         42       1251\n","4            to         39       2450\n","5            as         32        298\n","6         music         25       1598\n","7           for         17       1037\n","8            is         17       1330\n","9            be         16        351\n","10           on         15       1684\n","11  tchaikovsky         13       2369\n","12           by         13        409\n","13         that         12       2398\n","14          can         12        420\n","15          not         11       1649\n","16      between         11        376\n","17         such         11       2320\n","18   similarity         11       2188\n","19       models         10       1562"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["wf_list_K1 = dispAndMakeWordFreq(df,words, author = 1)\n","wf_list_K1 = wf_list_K1.sort_values('frequency', ascending=False)\n","wf_list_K1 = wf_list_K1.reset_index(drop=True)\n","wf_list_K1.head(20)"]},{"cell_type":"markdown","metadata":{},"source":["## Words Frequency of Dataset K2"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>words</th>\n","      <th>frequency</th>\n","      <th>wordIndex</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>the</td>\n","      <td>383</td>\n","      <td>2399</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>of</td>\n","      <td>161</td>\n","      <td>1678</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>to</td>\n","      <td>128</td>\n","      <td>2450</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>and</td>\n","      <td>121</td>\n","      <td>249</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>in</td>\n","      <td>119</td>\n","      <td>1251</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>is</td>\n","      <td>91</td>\n","      <td>1330</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>high</td>\n","      <td>76</td>\n","      <td>1178</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>eo</td>\n","      <td>56</td>\n","      <td>893</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>with</td>\n","      <td>56</td>\n","      <td>2633</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>for</td>\n","      <td>55</td>\n","      <td>1037</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>temperature</td>\n","      <td>53</td>\n","      <td>2378</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>at</td>\n","      <td>50</td>\n","      <td>311</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>gbit</td>\n","      <td>46</td>\n","      <td>1085</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>modulator</td>\n","      <td>45</td>\n","      <td>1569</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>device</td>\n","      <td>38</td>\n","      <td>741</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>are</td>\n","      <td>37</td>\n","      <td>285</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>sph</td>\n","      <td>36</td>\n","      <td>2247</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>as</td>\n","      <td>36</td>\n","      <td>298</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>polymer</td>\n","      <td>31</td>\n","      <td>1837</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>up</td>\n","      <td>29</td>\n","      <td>2529</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          words  frequency  wordIndex\n","0           the        383       2399\n","1            of        161       1678\n","2            to        128       2450\n","3           and        121        249\n","4            in        119       1251\n","5            is         91       1330\n","6          high         76       1178\n","7            eo         56        893\n","8          with         56       2633\n","9           for         55       1037\n","10  temperature         53       2378\n","11           at         50        311\n","12         gbit         46       1085\n","13    modulator         45       1569\n","14       device         38        741\n","15          are         37        285\n","16          sph         36       2247\n","17           as         36        298\n","18      polymer         31       1837\n","19           up         29       2529"]},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"source":["wf_list_K2 = dispAndMakeWordFreq(df,words, author = 2)\n","\n","wf_list_K2 = wf_list_K2.sort_values('frequency', ascending=False)\n","wf_list_K2 = wf_list_K2.reset_index(drop=True)\n","wf_list_K2.head(20)"]},{"cell_type":"markdown","metadata":{},"source":["## Words Frequency of Dataset ref"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>words</th>\n","      <th>frequency</th>\n","      <th>wordIndex</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>the</td>\n","      <td>183</td>\n","      <td>2399</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>of</td>\n","      <td>69</td>\n","      <td>1678</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>to</td>\n","      <td>58</td>\n","      <td>2450</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>in</td>\n","      <td>51</td>\n","      <td>1251</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>and</td>\n","      <td>29</td>\n","      <td>249</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>be</td>\n","      <td>25</td>\n","      <td>351</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>is</td>\n","      <td>25</td>\n","      <td>1330</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>will</td>\n","      <td>25</td>\n","      <td>2629</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>it</td>\n","      <td>22</td>\n","      <td>1335</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>administration</td>\n","      <td>19</td>\n","      <td>183</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>that</td>\n","      <td>15</td>\n","      <td>2398</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>tax</td>\n","      <td>15</td>\n","      <td>2367</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>with</td>\n","      <td>15</td>\n","      <td>2633</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>from</td>\n","      <td>14</td>\n","      <td>1061</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>not</td>\n","      <td>14</td>\n","      <td>1649</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>for</td>\n","      <td>14</td>\n","      <td>1037</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>by</td>\n","      <td>13</td>\n","      <td>409</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>legislature</td>\n","      <td>12</td>\n","      <td>1406</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>as</td>\n","      <td>11</td>\n","      <td>298</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>session</td>\n","      <td>11</td>\n","      <td>2150</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             words  frequency  wordIndex\n","0              the        183       2399\n","1               of         69       1678\n","2               to         58       2450\n","3               in         51       1251\n","4              and         29        249\n","5               be         25        351\n","6               is         25       1330\n","7             will         25       2629\n","8               it         22       1335\n","9   administration         19        183\n","10            that         15       2398\n","11             tax         15       2367\n","12            with         15       2633\n","13            from         14       1061\n","14             not         14       1649\n","15             for         14       1037\n","16              by         13        409\n","17     legislature         12       1406\n","18              as         11        298\n","19         session         11       2150"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["wf_list_ref = dispAndMakeWordFreq(df,words, author = 3)\n","wf_list_ref = wf_list_ref.sort_values('frequency', ascending=False)\n","wf_list_ref = wf_list_ref.reset_index(drop=True)\n","wf_list_ref.head(20)"]},{"cell_type":"markdown","metadata":{},"source":["## count, list and order the frequency of keywords\n","#Feature2"]},{"cell_type":"markdown","metadata":{},"source":["## Normalization of Word Frequencies to All Datasets and Add them into DF"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# we generaly name 'ntf' for normalized term frequency\n","normalized_tf_list = []\n","for row in df['tf']:\n","    num_words = sum(row)\n","    normalized_tf = []\n","    for x in row:\n","        normalized_tf.append(x/num_words)\n","    normalized_tf_list.append(normalized_tf)\n","\n","df['ntf'] = normalized_tf_list"]},{"cell_type":"markdown","metadata":{},"source":["#Tf-idf の代わりに利用する keyness を作る\n","\n","ここでは　df['keyness'] を作成し追加したい"]},{"cell_type":"markdown","metadata":{},"source":["## using Log ratio "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import math\n","# we generaly name 'ntf' for normalized term frequency\n","# First, create the shared normalized tf vector\n","# shared_ntf = None # shared ntf of all document\n","# matrix = []\n","# for row in df['ntf']:\n","#     matrix.append(row)\n","# np_matrix = np.array(matrix)\n","# mean_vector = np_matrix.mean(axis=0)\n","# shared_ntf = mean_vector.tolist()\n","\n","def keyness(ntf_vector1, ref_ntf_vector2): # freq_vector1 and freq_vector2 are both already normalized\n","    keyness_vec = []\n","    for i, x in enumerate(ntf_vector1):\n","        if ntf_vector1[i] == 0 or ref_ntf_vector2[i] == 0:\n","            keyness_vec.append(0)\n","        else:\n","            keyness_vec.append(math.log2(ntf_vector1[i]/ref_ntf_vector2[i]))\n","\n","    return keyness_vec\n","\n","\n","keyness_mat = []\n","for ntf_vector in df['ntf']:\n","    ntf_ref = df['ntf'][3]\n","    keyness_vec = keyness(ntf_vector, ntf_ref)\n","    keyness_mat.append(keyness_vec)\n","\n","# keyness を　Dataframe に追加\n","df['keyness'] = keyness_mat"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>author</th>\n","      <th>message</th>\n","      <th>tf</th>\n","      <th>ntf</th>\n","      <th>keyness</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Q DATASET</td>\n","      <td>\\n\\nHowever, there are frequent situations whe...</td>\n","      <td>[0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, ...</td>\n","      <td>[0.0, 0.0, 0.0, 0.0006980802792321117, 0.0, 0....</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, -1.5410720277427132, ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>K1 DATASET</td>\n","      <td>Download\\n\\nSource\\n\\nPDF\\nActions\\n   Copy Pr...</td>\n","      <td>[0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, ...</td>\n","      <td>[0.0, 0.0, 0.0, 0.000649772579597141, 0.000649...</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, -0.6445301204266565, ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>K2 DATASET</td>\n","      <td>\\n\\nWith the rapid growth of the information c...</td>\n","      <td>[2, 1, 1, 7, 22, 3, 2, 4, 1, 13, 4, 1, 6, 1, 0...</td>\n","      <td>[0.0004100041000410004, 0.0002050020500205002,...</td>\n","      <td>[-0.30882664689463013, 0, 0, 0, 0, 0, 0, 0, -2...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>BROWN DATASET</td>\n","      <td>Appointment of William S. Pfaff Jr. , 41 , as ...</td>\n","      <td>[1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 2, ...</td>\n","      <td>[0.0005078720162519045, 0.0, 0.0, 0.0, 0.0, 0....</td>\n","      <td>[0.0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0, 0, 0.0, 0, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          author                                            message  \\\n","0      Q DATASET  \\n\\nHowever, there are frequent situations whe...   \n","1     K1 DATASET  Download\\n\\nSource\\n\\nPDF\\nActions\\n   Copy Pr...   \n","2     K2 DATASET  \\n\\nWith the rapid growth of the information c...   \n","3  BROWN DATASET  Appointment of William S. Pfaff Jr. , 41 , as ...   \n","\n","                                                  tf  \\\n","0  [0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, ...   \n","1  [0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, ...   \n","2  [2, 1, 1, 7, 22, 3, 2, 4, 1, 13, 4, 1, 6, 1, 0...   \n","3  [1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 2, ...   \n","\n","                                                 ntf  \\\n","0  [0.0, 0.0, 0.0, 0.0006980802792321117, 0.0, 0....   \n","1  [0.0, 0.0, 0.0, 0.000649772579597141, 0.000649...   \n","2  [0.0004100041000410004, 0.0002050020500205002,...   \n","3  [0.0005078720162519045, 0.0, 0.0, 0.0, 0.0, 0....   \n","\n","                                             keyness  \n","0  [0, 0, 0, 0, 0, 0, 0, 0, -1.5410720277427132, ...  \n","1  [0, 0, 0, 0, 0, 0, 0, 0, -0.6445301204266565, ...  \n","2  [-0.30882664689463013, 0, 0, 0, 0, 0, 0, 0, -2...  \n","3  [0.0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0, 0, 0.0, 0, ...  "]},"execution_count":59,"metadata":{},"output_type":"execute_result"}],"source":["df"]},{"cell_type":"markdown","metadata":{},"source":["# Display the first 20 keywords of each dataset "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def dispAndMakeKeyWordList(df, words, author = 0):\n","    data = df.loc[author]\n","    keyness = data['keyness']\n","    wf = pd.DataFrame({'words': words, 'keyness': keyness})\n","    wordli = []\n","    freqli =[]\n","    wordindexli = []\n","    for key, data in wf.iterrows():\n","        if(int(data[1]) != 0):\n","            wordli.append(data[0])\n","            freqli.append(data[1])\n","            wordindexli.append(key)\n","        \n","    return pd.DataFrame({'words': wordli, 'keyness': freqli, 'wordIndex': wordindexli})"]},{"cell_type":"markdown","metadata":{},"source":["## Keyword of Dataset Q"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>words</th>\n","      <th>keyness</th>\n","      <th>wordIndex</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>services</td>\n","      <td>2.918360</td>\n","      <td>2149</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>time</td>\n","      <td>2.681320</td>\n","      <td>2441</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>when</td>\n","      <td>2.628853</td>\n","      <td>2617</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>related</td>\n","      <td>2.458928</td>\n","      <td>1994</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>major</td>\n","      <td>2.458928</td>\n","      <td>1467</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>using</td>\n","      <td>2.458928</td>\n","      <td>2542</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>work</td>\n","      <td>2.266283</td>\n","      <td>2639</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>or</td>\n","      <td>2.195894</td>\n","      <td>1715</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>while</td>\n","      <td>2.159368</td>\n","      <td>2621</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>use</td>\n","      <td>1.458928</td>\n","      <td>2536</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>through</td>\n","      <td>1.458928</td>\n","      <td>2434</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>day</td>\n","      <td>1.458928</td>\n","      <td>663</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>for</td>\n","      <td>1.432933</td>\n","      <td>1037</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>future</td>\n","      <td>1.266283</td>\n","      <td>1075</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>may</td>\n","      <td>1.266283</td>\n","      <td>1500</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>are</td>\n","      <td>1.243199</td>\n","      <td>285</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>decide</td>\n","      <td>1.043890</td>\n","      <td>672</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>depending</td>\n","      <td>1.043890</td>\n","      <td>713</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>only</td>\n","      <td>1.043890</td>\n","      <td>1689</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>re</td>\n","      <td>1.043890</td>\n","      <td>1955</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        words   keyness  wordIndex\n","0    services  2.918360       2149\n","1        time  2.681320       2441\n","2        when  2.628853       2617\n","3     related  2.458928       1994\n","4       major  2.458928       1467\n","5       using  2.458928       2542\n","6        work  2.266283       2639\n","7          or  2.195894       1715\n","8       while  2.159368       2621\n","9         use  1.458928       2536\n","10    through  1.458928       2434\n","11        day  1.458928        663\n","12        for  1.432933       1037\n","13     future  1.266283       1075\n","14        may  1.266283       1500\n","15        are  1.243199        285\n","16     decide  1.043890        672\n","17  depending  1.043890        713\n","18       only  1.043890       1689\n","19         re  1.043890       1955"]},"execution_count":61,"metadata":{},"output_type":"execute_result"}],"source":["keyword_list_Q = dispAndMakeKeyWordList(df,words, author = 0)\n","\n","keyword_list_Q = keyword_list_Q.sort_values('keyness',ascending=False)\n","keyword_list_Q = keyword_list_Q.reset_index(drop=True)\n","keyword_list_Q.head(20)"]},{"cell_type":"markdown","metadata":{},"source":["## Keyword of Dataset K1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>words</th>\n","      <th>keyness</th>\n","      <th>wordIndex</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>using</td>\n","      <td>2.940432</td>\n","      <td>2542</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>between</td>\n","      <td>2.814901</td>\n","      <td>376</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>us</td>\n","      <td>2.355470</td>\n","      <td>2535</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>work</td>\n","      <td>2.355470</td>\n","      <td>2639</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>harmony</td>\n","      <td>2.355470</td>\n","      <td>1156</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>though</td>\n","      <td>2.355470</td>\n","      <td>2427</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>like</td>\n","      <td>2.355470</td>\n","      <td>1421</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>how</td>\n","      <td>1.940432</td>\n","      <td>1201</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>still</td>\n","      <td>1.940432</td>\n","      <td>2278</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>these</td>\n","      <td>1.940432</td>\n","      <td>2415</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>case</td>\n","      <td>1.940432</td>\n","      <td>430</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>as</td>\n","      <td>1.896038</td>\n","      <td>298</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>can</td>\n","      <td>1.618504</td>\n","      <td>420</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>on</td>\n","      <td>1.455006</td>\n","      <td>1684</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>three</td>\n","      <td>1.355470</td>\n","      <td>2431</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>additional</td>\n","      <td>1.355470</td>\n","      <td>178</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>indeed</td>\n","      <td>1.355470</td>\n","      <td>1265</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>original</td>\n","      <td>1.355470</td>\n","      <td>1725</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>only</td>\n","      <td>1.355470</td>\n","      <td>1689</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>recent</td>\n","      <td>1.355470</td>\n","      <td>1967</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         words   keyness  wordIndex\n","0        using  2.940432       2542\n","1      between  2.814901        376\n","2           us  2.355470       2535\n","3         work  2.355470       2639\n","4      harmony  2.355470       1156\n","5       though  2.355470       2427\n","6         like  2.355470       1421\n","7          how  1.940432       1201\n","8        still  1.940432       2278\n","9        these  1.940432       2415\n","10        case  1.940432        430\n","11          as  1.896038        298\n","12         can  1.618504        420\n","13          on  1.455006       1684\n","14       three  1.355470       2431\n","15  additional  1.355470        178\n","16      indeed  1.355470       1265\n","17    original  1.355470       1725\n","18        only  1.355470       1689\n","19      recent  1.355470       1967"]},"execution_count":62,"metadata":{},"output_type":"execute_result"}],"source":["keyword_list_K1 = dispAndMakeKeyWordList(df,words, author = 1)\n","keyword_list_K1 = keyword_list_K1.sort_values('keyness',ascending=False)\n","keyword_list_K1 = keyword_list_K1.reset_index(drop=True)\n","keyword_list_K1.head(20)"]},{"cell_type":"markdown","metadata":{},"source":["## Keyword of Dataset K2"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>words</th>\n","      <th>keyness</th>\n","      <th>wordIndex</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>high</td>\n","      <td>3.939101</td>\n","      <td>1178</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>after</td>\n","      <td>2.778636</td>\n","      <td>198</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>power</td>\n","      <td>2.276136</td>\n","      <td>1847</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>test</td>\n","      <td>2.013101</td>\n","      <td>2391</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>up</td>\n","      <td>1.964192</td>\n","      <td>2529</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>when</td>\n","      <td>1.861098</td>\n","      <td>2617</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>increase</td>\n","      <td>1.498528</td>\n","      <td>1262</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>factor</td>\n","      <td>1.498528</td>\n","      <td>970</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>period</td>\n","      <td>1.498528</td>\n","      <td>1782</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>before</td>\n","      <td>1.276136</td>\n","      <td>359</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>using</td>\n","      <td>1.276136</td>\n","      <td>2542</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>at</td>\n","      <td>1.165105</td>\n","      <td>311</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>long</td>\n","      <td>1.106211</td>\n","      <td>1445</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>40</td>\n","      <td>1.013101</td>\n","      <td>89</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>order</td>\n","      <td>1.013101</td>\n","      <td>1716</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>integration</td>\n","      <td>-1.308827</td>\n","      <td>1304</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>given</td>\n","      <td>-1.308827</td>\n","      <td>1115</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>taking</td>\n","      <td>-1.308827</td>\n","      <td>2359</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>have</td>\n","      <td>-1.308827</td>\n","      <td>1160</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>highly</td>\n","      <td>-1.308827</td>\n","      <td>1181</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          words   keyness  wordIndex\n","0          high  3.939101       1178\n","1         after  2.778636        198\n","2         power  2.276136       1847\n","3          test  2.013101       2391\n","4            up  1.964192       2529\n","5          when  1.861098       2617\n","6      increase  1.498528       1262\n","7        factor  1.498528        970\n","8        period  1.498528       1782\n","9        before  1.276136        359\n","10        using  1.276136       2542\n","11           at  1.165105        311\n","12         long  1.106211       1445\n","13           40  1.013101         89\n","14        order  1.013101       1716\n","15  integration -1.308827       1304\n","16        given -1.308827       1115\n","17       taking -1.308827       2359\n","18         have -1.308827       1160\n","19       highly -1.308827       1181"]},"execution_count":63,"metadata":{},"output_type":"execute_result"}],"source":["keyword_list_K2 = dispAndMakeKeyWordList(df,words, author = 2)\n","\n","keyword_list_K2 = keyword_list_K2.sort_values('keyness',ascending=False)\n","keyword_list_K2 = keyword_list_K2.reset_index(drop=True)\n","keyword_list_K2.head(20)"]},{"cell_type":"markdown","metadata":{},"source":["# 7. display the shared words in the first 20 words of each dataset\n","#Feature7"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def dispAndMakeSharedWordsFreq(df,df_ref):\n","    df = df[df['words'].isin(df_ref['words'])] #filtering with the words in df2\n","    df_ref = df_ref[df_ref['words'].isin(df['words'])] #filtering with the words in df\n","    #now the words in df and df2 are same\n","    #sort words in the alphabetical order to become the same words as the same rows\n","    df = df.sort_values('words')\n","    df_ref = df_ref.sort_values('words')\n","    #merge df2 frequency to df1 \n","    df['ref_frequency'] = list(df_ref['frequency'])\n","    df['shared_word_keyword_frequency'] = (df['frequency'] + df['ref_frequency'])\n","    return df"]},{"cell_type":"markdown","metadata":{},"source":["## Shared Words Frequency in Dataset Q and K1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>words</th>\n","      <th>frequency</th>\n","      <th>wordIndex</th>\n","      <th>ref_frequency</th>\n","      <th>shared_word_keyword_frequency</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>the</td>\n","      <td>186</td>\n","      <td>2399</td>\n","      <td>78</td>\n","      <td>264</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>of</td>\n","      <td>89</td>\n","      <td>1678</td>\n","      <td>66</td>\n","      <td>155</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>and</td>\n","      <td>74</td>\n","      <td>249</td>\n","      <td>50</td>\n","      <td>124</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>to</td>\n","      <td>70</td>\n","      <td>2450</td>\n","      <td>39</td>\n","      <td>109</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>for</td>\n","      <td>55</td>\n","      <td>1037</td>\n","      <td>17</td>\n","      <td>72</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>in</td>\n","      <td>54</td>\n","      <td>1251</td>\n","      <td>42</td>\n","      <td>96</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>be</td>\n","      <td>45</td>\n","      <td>351</td>\n","      <td>16</td>\n","      <td>61</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>or</td>\n","      <td>40</td>\n","      <td>1715</td>\n","      <td>5</td>\n","      <td>45</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>is</td>\n","      <td>31</td>\n","      <td>1330</td>\n","      <td>17</td>\n","      <td>48</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>are</td>\n","      <td>31</td>\n","      <td>285</td>\n","      <td>4</td>\n","      <td>35</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>as</td>\n","      <td>28</td>\n","      <td>298</td>\n","      <td>32</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>time</td>\n","      <td>28</td>\n","      <td>2441</td>\n","      <td>1</td>\n","      <td>29</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>based</td>\n","      <td>20</td>\n","      <td>347</td>\n","      <td>6</td>\n","      <td>26</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>that</td>\n","      <td>20</td>\n","      <td>2398</td>\n","      <td>12</td>\n","      <td>32</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>system</td>\n","      <td>19</td>\n","      <td>2351</td>\n","      <td>3</td>\n","      <td>22</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>by</td>\n","      <td>18</td>\n","      <td>409</td>\n","      <td>13</td>\n","      <td>31</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>such</td>\n","      <td>18</td>\n","      <td>2320</td>\n","      <td>11</td>\n","      <td>29</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>on</td>\n","      <td>18</td>\n","      <td>1684</td>\n","      <td>15</td>\n","      <td>33</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>figure</td>\n","      <td>18</td>\n","      <td>994</td>\n","      <td>9</td>\n","      <td>27</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>with</td>\n","      <td>17</td>\n","      <td>2633</td>\n","      <td>9</td>\n","      <td>26</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     words  frequency  wordIndex  ref_frequency  shared_word_keyword_frequency\n","0      the        186       2399             78                            264\n","1       of         89       1678             66                            155\n","2      and         74        249             50                            124\n","3       to         70       2450             39                            109\n","4      for         55       1037             17                             72\n","5       in         54       1251             42                             96\n","6       be         45        351             16                             61\n","7       or         40       1715              5                             45\n","8       is         31       1330             17                             48\n","9      are         31        285              4                             35\n","10      as         28        298             32                             60\n","11    time         28       2441              1                             29\n","12   based         20        347              6                             26\n","13    that         20       2398             12                             32\n","14  system         19       2351              3                             22\n","15      by         18        409             13                             31\n","16    such         18       2320             11                             29\n","17      on         18       1684             15                             33\n","18  figure         18        994              9                             27\n","19    with         17       2633              9                             26"]},"execution_count":65,"metadata":{},"output_type":"execute_result"}],"source":["#SWF = Shared Word Frequency\n","SWF_QandK1 = dispAndMakeSharedWordsFreq(wf_list_Q,wf_list_K1)\n","SWF_QandK1=SWF_QandK1.sort_values('frequency', ascending=False)\n","SWF_QandK1 = SWF_QandK1.reset_index(drop=True)\n","SWF_QandK1.head(20)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Shared Words Frequency in Dataset Q and K2"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>words</th>\n","      <th>frequency</th>\n","      <th>wordIndex</th>\n","      <th>ref_frequency</th>\n","      <th>shared_word_keyword_frequency</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>the</td>\n","      <td>186</td>\n","      <td>2399</td>\n","      <td>383</td>\n","      <td>569</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>of</td>\n","      <td>89</td>\n","      <td>1678</td>\n","      <td>161</td>\n","      <td>250</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>and</td>\n","      <td>74</td>\n","      <td>249</td>\n","      <td>121</td>\n","      <td>195</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>to</td>\n","      <td>70</td>\n","      <td>2450</td>\n","      <td>128</td>\n","      <td>198</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>for</td>\n","      <td>55</td>\n","      <td>1037</td>\n","      <td>55</td>\n","      <td>110</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>in</td>\n","      <td>54</td>\n","      <td>1251</td>\n","      <td>119</td>\n","      <td>173</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>be</td>\n","      <td>45</td>\n","      <td>351</td>\n","      <td>19</td>\n","      <td>64</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>or</td>\n","      <td>40</td>\n","      <td>1715</td>\n","      <td>10</td>\n","      <td>50</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>is</td>\n","      <td>31</td>\n","      <td>1330</td>\n","      <td>91</td>\n","      <td>122</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>are</td>\n","      <td>31</td>\n","      <td>285</td>\n","      <td>37</td>\n","      <td>68</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>time</td>\n","      <td>28</td>\n","      <td>2441</td>\n","      <td>1</td>\n","      <td>29</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>as</td>\n","      <td>28</td>\n","      <td>298</td>\n","      <td>36</td>\n","      <td>64</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>based</td>\n","      <td>20</td>\n","      <td>347</td>\n","      <td>3</td>\n","      <td>23</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>that</td>\n","      <td>20</td>\n","      <td>2398</td>\n","      <td>7</td>\n","      <td>27</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>system</td>\n","      <td>19</td>\n","      <td>2351</td>\n","      <td>3</td>\n","      <td>22</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>figure</td>\n","      <td>18</td>\n","      <td>994</td>\n","      <td>5</td>\n","      <td>23</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>by</td>\n","      <td>18</td>\n","      <td>409</td>\n","      <td>26</td>\n","      <td>44</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>such</td>\n","      <td>18</td>\n","      <td>2320</td>\n","      <td>16</td>\n","      <td>34</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>on</td>\n","      <td>18</td>\n","      <td>1684</td>\n","      <td>26</td>\n","      <td>44</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>it</td>\n","      <td>17</td>\n","      <td>1335</td>\n","      <td>10</td>\n","      <td>27</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     words  frequency  wordIndex  ref_frequency  shared_word_keyword_frequency\n","0      the        186       2399            383                            569\n","1       of         89       1678            161                            250\n","2      and         74        249            121                            195\n","3       to         70       2450            128                            198\n","4      for         55       1037             55                            110\n","5       in         54       1251            119                            173\n","6       be         45        351             19                             64\n","7       or         40       1715             10                             50\n","8       is         31       1330             91                            122\n","9      are         31        285             37                             68\n","10    time         28       2441              1                             29\n","11      as         28        298             36                             64\n","12   based         20        347              3                             23\n","13    that         20       2398              7                             27\n","14  system         19       2351              3                             22\n","15  figure         18        994              5                             23\n","16      by         18        409             26                             44\n","17    such         18       2320             16                             34\n","18      on         18       1684             26                             44\n","19      it         17       1335             10                             27"]},"execution_count":66,"metadata":{},"output_type":"execute_result"}],"source":["#SWF = Shared Word Frequency\n","SWF_QandK2 = dispAndMakeSharedWordsFreq(wf_list_Q,wf_list_K2)\n","SWF_QandK2 = SWF_QandK2.sort_values('frequency', ascending=False)\n","SWF_QandK2 = SWF_QandK2.reset_index(drop=True)\n","SWF_QandK2.head(20)"]},{"cell_type":"markdown","metadata":{},"source":["## Display the shared keywords in the first 20 keywords of each dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def dispAndMakeSharedKeyword(df,df_ref):\n","    df = df[df['words'].isin(df_ref['words'])] #filtering with the words in df2\n","    df_ref = df_ref[df_ref['words'].isin(df['words'])] #filtering with the words in df\n","    #now the words in df and df2 are same\n","    #sort words in the alphabetical order to become the same words as the same rows\n","    df = df.sort_values('words')\n","    df_ref = df_ref.sort_values('words')\n","    #merge df2 frequency to df1 \n","    df['ref_keyness'] = list(df_ref['keyness'])\n","    df['wordIndex'] = list(df['wordIndex'])\n","    \n","    return df"]},{"cell_type":"markdown","metadata":{},"source":["## Shared Keywords in Dataset Q and K1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>words</th>\n","      <th>keyness</th>\n","      <th>wordIndex</th>\n","      <th>ref_keyness</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>time</td>\n","      <td>2.681320</td>\n","      <td>2441</td>\n","      <td>-1.229493</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>using</td>\n","      <td>2.458928</td>\n","      <td>2542</td>\n","      <td>2.940432</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>work</td>\n","      <td>2.266283</td>\n","      <td>2639</td>\n","      <td>2.355470</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>only</td>\n","      <td>1.043890</td>\n","      <td>1689</td>\n","      <td>1.355470</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>though</td>\n","      <td>1.043890</td>\n","      <td>2427</td>\n","      <td>2.355470</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>give</td>\n","      <td>-1.126035</td>\n","      <td>1114</td>\n","      <td>-1.229493</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>well</td>\n","      <td>-1.126035</td>\n","      <td>2614</td>\n","      <td>1.355470</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>no</td>\n","      <td>-1.126035</td>\n","      <td>1640</td>\n","      <td>-1.229493</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>part</td>\n","      <td>-1.126035</td>\n","      <td>1751</td>\n","      <td>-1.229493</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>they</td>\n","      <td>-1.278038</td>\n","      <td>2416</td>\n","      <td>-2.966458</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>about</td>\n","      <td>-1.541072</td>\n","      <td>137</td>\n","      <td>-1.644530</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>many</td>\n","      <td>-1.863000</td>\n","      <td>1480</td>\n","      <td>-1.966458</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>two</td>\n","      <td>-2.126035</td>\n","      <td>2501</td>\n","      <td>-1.229493</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>than</td>\n","      <td>-2.126035</td>\n","      <td>2397</td>\n","      <td>-1.229493</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>transition</td>\n","      <td>-2.126035</td>\n","      <td>2473</td>\n","      <td>-1.229493</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>also</td>\n","      <td>-2.126035</td>\n","      <td>229</td>\n","      <td>-1.229493</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>would</td>\n","      <td>-2.278038</td>\n","      <td>2646</td>\n","      <td>-2.966458</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>if</td>\n","      <td>-2.348427</td>\n","      <td>1220</td>\n","      <td>-2.451885</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>from</td>\n","      <td>-3.348427</td>\n","      <td>1061</td>\n","      <td>-1.451885</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         words   keyness  wordIndex  ref_keyness\n","0         time  2.681320       2441    -1.229493\n","1        using  2.458928       2542     2.940432\n","2         work  2.266283       2639     2.355470\n","3         only  1.043890       1689     1.355470\n","4       though  1.043890       2427     2.355470\n","5         give -1.126035       1114    -1.229493\n","6         well -1.126035       2614     1.355470\n","7           no -1.126035       1640    -1.229493\n","8         part -1.126035       1751    -1.229493\n","9         they -1.278038       2416    -2.966458\n","10       about -1.541072        137    -1.644530\n","11        many -1.863000       1480    -1.966458\n","12         two -2.126035       2501    -1.229493\n","13        than -2.126035       2397    -1.229493\n","14  transition -2.126035       2473    -1.229493\n","15        also -2.126035        229    -1.229493\n","16       would -2.278038       2646    -2.966458\n","17          if -2.348427       1220    -2.451885\n","18        from -3.348427       1061    -1.451885"]},"execution_count":68,"metadata":{},"output_type":"execute_result"}],"source":["SK_QandK1 = dispAndMakeSharedKeyword(keyword_list_Q,keyword_list_K1)\n","SK_QandK1 = SK_QandK1.sort_values('keyness', ascending=False)\n","SK_QandK1 = SK_QandK1.reset_index(drop=True)\n","SK_QandK1.head(20)"]},{"cell_type":"markdown","metadata":{},"source":["## Shared Keywords in Dataset Q and K2"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>words</th>\n","      <th>keyness</th>\n","      <th>wordIndex</th>\n","      <th>ref_keyness</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>time</td>\n","      <td>2.681320</td>\n","      <td>2441</td>\n","      <td>-2.893789</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>when</td>\n","      <td>2.628853</td>\n","      <td>2617</td>\n","      <td>1.861098</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>using</td>\n","      <td>2.458928</td>\n","      <td>2542</td>\n","      <td>1.276136</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>may</td>\n","      <td>1.266283</td>\n","      <td>1500</td>\n","      <td>-3.308827</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>stage</td>\n","      <td>1.043890</td>\n","      <td>2259</td>\n","      <td>-1.308827</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>make</td>\n","      <td>-1.126035</td>\n","      <td>1468</td>\n","      <td>-2.893789</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>12</td>\n","      <td>-1.541072</td>\n","      <td>11</td>\n","      <td>-2.308827</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>11</td>\n","      <td>-1.541072</td>\n","      <td>8</td>\n","      <td>-2.308827</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>high</td>\n","      <td>-1.541072</td>\n","      <td>1178</td>\n","      <td>3.939101</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>about</td>\n","      <td>-1.541072</td>\n","      <td>137</td>\n","      <td>-3.308827</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>into</td>\n","      <td>-1.863000</td>\n","      <td>1314</td>\n","      <td>-2.045792</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>all</td>\n","      <td>-1.863000</td>\n","      <td>219</td>\n","      <td>-2.630755</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>see</td>\n","      <td>-2.126035</td>\n","      <td>2125</td>\n","      <td>-1.893789</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>together</td>\n","      <td>-2.126035</td>\n","      <td>2452</td>\n","      <td>-1.893789</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>transition</td>\n","      <td>-2.126035</td>\n","      <td>2473</td>\n","      <td>-1.308827</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>two</td>\n","      <td>-2.126035</td>\n","      <td>2501</td>\n","      <td>-1.308827</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>would</td>\n","      <td>-2.278038</td>\n","      <td>2646</td>\n","      <td>-4.630755</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>most</td>\n","      <td>-2.348427</td>\n","      <td>1581</td>\n","      <td>-2.531219</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>issue</td>\n","      <td>-2.541072</td>\n","      <td>1332</td>\n","      <td>-2.308827</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>its</td>\n","      <td>-2.541072</td>\n","      <td>1339</td>\n","      <td>-1.723864</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         words   keyness  wordIndex  ref_keyness\n","0         time  2.681320       2441    -2.893789\n","1         when  2.628853       2617     1.861098\n","2        using  2.458928       2542     1.276136\n","3          may  1.266283       1500    -3.308827\n","4        stage  1.043890       2259    -1.308827\n","5         make -1.126035       1468    -2.893789\n","6           12 -1.541072         11    -2.308827\n","7           11 -1.541072          8    -2.308827\n","8         high -1.541072       1178     3.939101\n","9        about -1.541072        137    -3.308827\n","10        into -1.863000       1314    -2.045792\n","11         all -1.863000        219    -2.630755\n","12         see -2.126035       2125    -1.893789\n","13    together -2.126035       2452    -1.893789\n","14  transition -2.126035       2473    -1.308827\n","15         two -2.126035       2501    -1.308827\n","16       would -2.278038       2646    -4.630755\n","17        most -2.348427       1581    -2.531219\n","18       issue -2.541072       1332    -2.308827\n","19         its -2.541072       1339    -1.723864"]},"execution_count":69,"metadata":{},"output_type":"execute_result"}],"source":["SK_QandK2 = dispAndMakeSharedKeyword(keyword_list_Q,keyword_list_K2)\n","SK_QandK2 = SK_QandK2.sort_values('keyness', ascending=False)\n","SK_QandK2 = SK_QandK2.reset_index(drop=True)\n","SK_QandK2.head(20)"]},{"cell_type":"markdown","metadata":{},"source":["# Remake Keyness values and Shared Words Frequency for Each Author "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Only filterling the keyness and SFW values using sheared word freq and shared keyword.\n","#this objective is to adjust the length of shared keyword and word list to original size of 'words'\n","def remakeKeynessAndFW(df, words,new_SWF,new_keyness, authorId = 0 ):\n","    if(authorId==0):\n","        print('dataset Q is not allowed')\n","        return [],[]\n","    #init list by 0\n","    keynessli=[]\n","    SFWli = []\n","    for k in range(len(words)):\n","        keynessli.append(0)\n","        SFWli.append(0)\n","    #end init\n","    #make new shared word frequency list\n","    #items[] compounds of: words,\tfrequency,\twordIndex,\tref_frequency\n","    for key,items in new_SWF.iterrows():\n","        SFWli[items[2]] = items[3]\n","    #make new shared keyword list\n","    # items[] compounds of: words,\tkeyness,\twordIndex,\tref_keyness\n","    for key, items in new_keyness.iterrows():\n","        keynessli[items[2]] = items[3]\n","    return SFWli, keynessli"]},{"cell_type":"markdown","metadata":{},"source":["### Now K1 and K2 in df have the tf and keyness based on the existence of each shared word freq. and shared keywords as new datasets"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df['tf'][1], df['keyness'][1] = remakeKeynessAndFW(df, words, SWF_QandK1, SK_QandK1, 1)\n","df['tf'][2], df['keyness'][2] = remakeKeynessAndFW(df, words, SWF_QandK2, SK_QandK2, 2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>author</th>\n","      <th>message</th>\n","      <th>tf</th>\n","      <th>ntf</th>\n","      <th>keyness</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Q DATASET</td>\n","      <td>\\n\\nHowever, there are frequent situations whe...</td>\n","      <td>[0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, ...</td>\n","      <td>[0.0, 0.0, 0.0, 0.0006980802792321117, 0.0, 0....</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, -1.5410720277427132, ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>K1 DATASET</td>\n","      <td>Download\\n\\nSource\\n\\nPDF\\nActions\\n   Copy Pr...</td>\n","      <td>[0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, ...</td>\n","      <td>[0.0, 0.0, 0.0, 0.000649772579597141, 0.000649...</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>K2 DATASET</td>\n","      <td>\\n\\nWith the rapid growth of the information c...</td>\n","      <td>[0, 0, 0, 7, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, ...</td>\n","      <td>[0.0004100041000410004, 0.0002050020500205002,...</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, -2.30882664689463, 0,...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>BROWN DATASET</td>\n","      <td>Appointment of William S. Pfaff Jr. , 41 , as ...</td>\n","      <td>[1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 2, ...</td>\n","      <td>[0.0005078720162519045, 0.0, 0.0, 0.0, 0.0, 0....</td>\n","      <td>[0.0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0, 0, 0.0, 0, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          author                                            message  \\\n","0      Q DATASET  \\n\\nHowever, there are frequent situations whe...   \n","1     K1 DATASET  Download\\n\\nSource\\n\\nPDF\\nActions\\n   Copy Pr...   \n","2     K2 DATASET  \\n\\nWith the rapid growth of the information c...   \n","3  BROWN DATASET  Appointment of William S. Pfaff Jr. , 41 , as ...   \n","\n","                                                  tf  \\\n","0  [0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, ...   \n","1  [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, ...   \n","2  [0, 0, 0, 7, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, ...   \n","3  [1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 2, ...   \n","\n","                                                 ntf  \\\n","0  [0.0, 0.0, 0.0, 0.0006980802792321117, 0.0, 0....   \n","1  [0.0, 0.0, 0.0, 0.000649772579597141, 0.000649...   \n","2  [0.0004100041000410004, 0.0002050020500205002,...   \n","3  [0.0005078720162519045, 0.0, 0.0, 0.0, 0.0, 0....   \n","\n","                                             keyness  \n","0  [0, 0, 0, 0, 0, 0, 0, 0, -1.5410720277427132, ...  \n","1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n","2  [0, 0, 0, 0, 0, 0, 0, 0, -2.30882664689463, 0,...  \n","3  [0.0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0, 0, 0.0, 0, ...  "]},"execution_count":72,"metadata":{},"output_type":"execute_result"}],"source":["df"]},{"cell_type":"markdown","metadata":{"id":"XYWb-aWPxIys"},"source":["# Prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"byj_PcU4dYTz"},"outputs":[],"source":["# # start からend までのwordの配列を返す\n","# def extract_features_words(freq_vector, words, start=0, end=20):\n","#     setX = set(freq_vector[0]) # 最大値を取り出すため set を作成\n","#     count = 0\n","#     result = []\n","#     while count<end:\n","#         max_value = max(setX)\n","#         max_index = freq_vector.index(max_value)\n","#         max_word = words[max_index]\n","#         setX.remove(max_value)\n","#         ### if exclude stopwords\n","#         # if max_word not in stop_words:\n","#         #     if count>= start:\n","#         #         result.append(max_index)\n","#         #     count+=1\n","#         if count>= start:\n","#             result.append(max_word)\n","#         count += 1\n","#     return result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6sUvSp9pdYT1"},"outputs":[],"source":["# start からend までのword IDの配列を返す\n","def extract_features(freq_vector, words, start=0, end=20):\n","    freq_vector = freq_vector[0].copy()\n","    setX = freq_vector # 最大値を取り出すため set を作成\n","    count = 0\n","    result=[]\n","    while count<end:\n","        try:\n","            max_value = max(setX)\n","            # print(max_value)\n","        except ValueError:\n","            print('valueerror')\n","            return result\n","        max_index = freq_vector.index(max_value)\n","        max_word = words[max_index]\n","        setX.remove(max_value)\n","        if count>= start:\n","            result.append(max_word)\n","        count += 1\n","        ### if exclude stopwords\n","        # if max_word not in stop_words:\n","        #     if count>= start:\n","        #         result.append(max_index)\n","        #     count+=1\n","\n","        \n","    return result\n","\n","#testcase\n","\n","# extract_features(df['tf'].tolist(), words, 0 , 20)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xWhTAjQYkwOy"},"outputs":[],"source":["def get_similarity(feature_vector1,feature_vector2):\n","    return len(set(feature_vector1) & set(feature_vector2))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BBTAic9Mdytb"},"outputs":[],"source":["INF = float('inf')\n","\n","def predict(questioned_vector,candidates_vectors):\n","    #initialize-------------------------------------\n","    start = 0\n","    end = 20\n","    similarityWithQ_tf = {}\n","    similarityWithQ_keyness = {}\n","    suspected = list(candidates_vectors['author'])\n","    #prepare questioned tf and keyword features\n","    \n","    while(len(suspected) > 1):\n","        Q_features_tf = extract_features(questioned_vector['tf'].tolist(), words, start, end)\n","        Q_features_keyness = extract_features(questioned_vector['keyness'].tolist(), words, start, end)\n","        for idx, candidates_items in candidates_vectors.iterrows():\n","            author = candidates_items[0]\n","            \n","            #prepare candidates tf and keynes features\n","            candidates_vector_tf = [candidates_items[2]] \n","            candidates_vector_keyness = [candidates_items[4]]\n","            if author in suspected:\n","                print('Analysed Author Information')\n","                #tf-------------------\n","                C_features_tf = extract_features(candidates_vector_tf, words, start, end)\n","                score_tf = get_similarity(C_features_tf,Q_features_tf)\n","                similarityWithQ_tf[author]=score_tf\n","                print(f'{author}\\'s similality tf score = {score_tf}')\n","                \n","                #keyness----------------\n","                C_features_keyness = extract_features(candidates_vector_keyness, words, start, end)\n","                score_keyness = get_similarity(C_features_keyness,Q_features_keyness)\n","                similarityWithQ_keyness[author]=score_keyness\n","                print(f'{author}\\'s similality keyness score = {score_keyness}')\n","                print('...')\n","        innocent = min(similarityWithQ_keyness, key=similarityWithQ_keyness.get)\n","        suspect =  max(similarityWithQ_tf, key=similarityWithQ_tf.get)\n","\n","        #asking what they want to do\n","        act = 0\n","        while(1):\n","            if(len(suspected) == 1):\n","                print('*********************************')\n","                print(f'Final Result of the suspectful auther is: {suspect}')\n","                print('Thank you.')\n","                print('*********************************')\n","                return suspect\n","            print('*********************************')\n","            print(f'The MOST suspectful auther based on shared keyword frequency: {suspect}')\n","            print(f'The LEAST suspectful auther based on Keyness: {innocent}')\n","            print('*********************************')\n","            act = input(f'Do you wan to remove the LEAST suspectful one from searching, \\\"{innocent}\\\"?(yes: 1, no: 0)')\n","            try:\n","                if(int(act) == 1):\n","                    print('...')\n","                    print('remove the user from candidates')\n","                    suspected.remove(innocent)\n","                else:\n","                    print('...')\n","                    print('Go to next 20 words searching')\n","                    break\n","            except:\n","                print('Please input decimal number\\n')\n","                break\n","        end += 20\n","\n","    return suspected[0]"]},{"cell_type":"markdown","metadata":{},"source":["### Divided df into Q and Candidates"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>author</th>\n","      <th>message</th>\n","      <th>tf</th>\n","      <th>ntf</th>\n","      <th>keyness</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Q DATASET</td>\n","      <td>\\n\\nHowever, there are frequent situations whe...</td>\n","      <td>[0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, ...</td>\n","      <td>[0.0, 0.0, 0.0, 0.0006980802792321117, 0.0, 0....</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, -1.5410720277427132, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      author                                            message  \\\n","0  Q DATASET  \\n\\nHowever, there are frequent situations whe...   \n","\n","                                                  tf  \\\n","0  [0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, ...   \n","\n","                                                 ntf  \\\n","0  [0.0, 0.0, 0.0, 0.0006980802792321117, 0.0, 0....   \n","\n","                                             keyness  \n","0  [0, 0, 0, 0, 0, 0, 0, 0, -1.5410720277427132, ...  "]},"execution_count":77,"metadata":{},"output_type":"execute_result"}],"source":["questioned_df = df[0:1].copy()\n","references_df = df[1:3].copy()\n","references_df.reset_index\n","questioned_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>author</th>\n","      <th>message</th>\n","      <th>tf</th>\n","      <th>ntf</th>\n","      <th>keyness</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>K1 DATASET</td>\n","      <td>Download\\n\\nSource\\n\\nPDF\\nActions\\n   Copy Pr...</td>\n","      <td>[0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, ...</td>\n","      <td>[0.0, 0.0, 0.0, 0.000649772579597141, 0.000649...</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>K2 DATASET</td>\n","      <td>\\n\\nWith the rapid growth of the information c...</td>\n","      <td>[0, 0, 0, 7, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, ...</td>\n","      <td>[0.0004100041000410004, 0.0002050020500205002,...</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, -2.30882664689463, 0,...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       author                                            message  \\\n","1  K1 DATASET  Download\\n\\nSource\\n\\nPDF\\nActions\\n   Copy Pr...   \n","2  K2 DATASET  \\n\\nWith the rapid growth of the information c...   \n","\n","                                                  tf  \\\n","1  [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, ...   \n","2  [0, 0, 0, 7, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, ...   \n","\n","                                                 ntf  \\\n","1  [0.0, 0.0, 0.0, 0.000649772579597141, 0.000649...   \n","2  [0.0004100041000410004, 0.0002050020500205002,...   \n","\n","                                             keyness  \n","1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n","2  [0, 0, 0, 0, 0, 0, 0, 0, -2.30882664689463, 0,...  "]},"execution_count":78,"metadata":{},"output_type":"execute_result"}],"source":["references_df"]},{"cell_type":"code","execution_count":79,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Analysed Author Information\n","K1 DATASET's similality tf score = 4\n","K1 DATASET's similality keyness score = 0\n","...\n","Analysed Author Information\n","K2 DATASET's similality tf score = 6\n","K2 DATASET's similality keyness score = 0\n","...\n","*********************************\n","The MOST suspectful auther based on shared keyword frequency: K2 DATASET\n","The LEAST suspectful auther based on Keyness: K1 DATASET\n","*********************************\n","Please input decimal number\n","\n","Analysed Author Information\n","K1 DATASET's similality tf score = 6\n","K1 DATASET's similality keyness score = 0\n","...\n","Analysed Author Information\n","K2 DATASET's similality tf score = 7\n","K2 DATASET's similality keyness score = 1\n","...\n","*********************************\n","The MOST suspectful auther based on shared keyword frequency: K2 DATASET\n","The LEAST suspectful auther based on Keyness: K1 DATASET\n","*********************************\n","Please input decimal number\n","\n","Analysed Author Information\n","K1 DATASET's similality tf score = 7\n","K1 DATASET's similality keyness score = 0\n","...\n","Analysed Author Information\n","K2 DATASET's similality tf score = 7\n","K2 DATASET's similality keyness score = 1\n","...\n","*********************************\n","The MOST suspectful auther based on shared keyword frequency: K1 DATASET\n","The LEAST suspectful auther based on Keyness: K1 DATASET\n","*********************************\n","Please input decimal number\n","\n","Analysed Author Information\n","K1 DATASET's similality tf score = 8\n","K1 DATASET's similality keyness score = 1\n","...\n","Analysed Author Information\n","K2 DATASET's similality tf score = 7\n","K2 DATASET's similality keyness score = 5\n","...\n","*********************************\n","The MOST suspectful auther based on shared keyword frequency: K1 DATASET\n","The LEAST suspectful auther based on Keyness: K1 DATASET\n","*********************************\n"]}],"source":["suspect = predict(questioned_df, references_df)\n","# suspect = predict(questioned_df, df)\n"]},{"cell_type":"markdown","metadata":{},"source":["# 没キーワード頻度取得関数等"]},{"cell_type":"markdown","metadata":{"id":"BJVfCH7wy3LE"},"source":["## Check function in Train Data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":313,"status":"ok","timestamp":1689128019130,"user":{"displayName":"米田武士","userId":"14881788265874075884"},"user_tz":-540},"id":"1PsLfZIkxIyx","outputId":"6a3aaede-d05d-42ba-a843-7e031f4f2869"},"outputs":[{"ename":"NameError","evalue":"name 'reference_vectors' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[50], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m----> 2\u001b[0m bad_guy \u001b[39m=\u001b[39m predict(df[\u001b[39m'\u001b[39m\u001b[39mkeyness\u001b[39m\u001b[39m'\u001b[39m][i], reference_vectors)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbad guy : \u001b[39m\u001b[39m{\u001b[39;00mbad_guy\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m*\u001b[39m\u001b[39m20\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'reference_vectors' is not defined"]}],"source":["i = 0\n","bad_guy = predict(df['keyness'][i], reference_vectors)\n","print(f'bad guy : {bad_guy}')\n","print('-'*20)\n","print(f\"True author : {df['author'][i]}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rHK8sTcD8RSV"},"outputs":[],"source":["# all_test_data = len(X_keyness_test)# "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":341225,"status":"ok","timestamp":1689129589788,"user":{"displayName":"米田武士","userId":"14881788265874075884"},"user_tz":-540},"id":"Q3atvZGPkAP7","outputId":"87a4553b-1602-41c0-8292-f5fb7f16cd10"},"outputs":[],"source":["# match_cnt = 0\n","# all_test_data = len(X_keyness_train)\n","# for i in X_keyness_train.index:\n","#     bad_guy = predict(df['keyness'][i], reference_vectors)\n","#     if df['author'][i] == bad_guy:\n","#         match_cnt = match_cnt + 1\n","\n","# print(f'Math rate is: {match_cnt/all_test_data*100} % ')\n","#     #print(f'bad guy : {bad_guy}')\n","#     #print('-'*20)\n","#     #print(f\"True author : {df['author'][i]}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t7ltZakCdYT6"},"outputs":[],"source":["# def predict(questioned_vector, reference_vectors):\n","#     suspected = [author for author in authors]\n","\n","#     comparedSize = 20\n","#     while(len(suspected) > 1):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X6PHDHBmdYT-"},"outputs":[],"source":["# def dispAndMakeSharedKeywordFreq(df,df_ref):\n","#     df = df[df['words'].isin(df_ref['words'])] #filtering with the words in df2\n","#     df_ref = df_ref[df_ref['words'].isin(df['words'])] #filtering with the words in df\n","#     #now the words in df and df2 are same\n","#     #sort words in the alphabetical order to become the same words as the same rows\n","#     df = df.sort_values('words')\n","#     df_ref = df_ref.sort_values('words')\n","#     #merge df2 frequency to df1 \n","#     df['ref_frequency'] = list(df_ref['frequency'])\n","#     df['shared_word_keyword_frequency'] = (df['frequency'] + df['ref_frequency'])\n","#     return df\n","## Keyword Frequency in Dataset Q and Ref\n","#KF = Keyword Frequency\n","# KF_QandRef = dispAndMakeSharedKeywordFreq(wf_list_Q,wf_list_ref)\n","# KF_QandRef.sort_values('frequency', ascending=False).head(20)\n","# ## Keyword Frequency in Dataset K1 and Ref\n","# #KF = Shared Keyword Frequency\n","# KF_QandRef = dispAndMakeSharedKeywordFreq(wf_list_K1,wf_list_ref)\n","# KF_QandRef.sort_values('frequency', ascending=False).head(20)\n","# ## Keyword Frequency in Dataset K2 and Ref\n","# #KF = Shared Keyword Frequency\n","# KF_QandRef = dispAndMakeSharedKeywordFreq(wf_list_K2,wf_list_ref)\n","# KF_QandRef.sort_values('frequency', ascending=False).head(20)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# import nltk\n","# from nltk.corpus import stopwords\n","\n","# nltk.download('stopwords')\n","# stop_words = stopwords.words('english')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# #確認用\n","# i = 22\n","# msg = df['message'][i]\n","# max_value = max(keyness_mat[i])\n","# max_idx = keyness_mat[i].index(max_value)\n","# print(words[max_idx])\n","\n","# print(msg)\n","# #print(df['author'][i])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# from sklearn.model_selection import train_test_split\n","# X_keyness_train, X_keyness_test, Y_keyness_train, Y_keyness_test = train_test_split(df['keyness'],df['author'],test_size=0.2,shuffle=True)\n","# X_tf_train, X_tf_test, Y_tf_train, Y_tf_test = train_test_split(df['tf'],df['author'],test_size=0.2,shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # How many author?\n","# authors = set(Y_keyness_test)\n","# authors_list = [author for author in authors]"]},{"cell_type":"markdown","metadata":{},"source":["# Create Reference_vectors\n","size: 著者の数\n","\n","Train データから作る"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # df_X = pd.DataFrame(X_tf_train.values.tolist())\n","# # df_Y = pd.DataFrame(Y_tf_train.values.tolist())\n","\n","\n","# df_train = pd.concat((X_keyness_train, Y_keyness_train.rename('author')), axis=1)\n","# #\n","# reference_vectors = {}\n","# for author in authors:\n","\n","#     df_author = df_train.groupby('author').get_group(author)\n","\n","#     matrix = []\n","#     for row in df_author['keyness']:\n","#         matrix.append(row)\n","\n","#     np_matrix = np.array(matrix)\n","\n","#     mean_vector = np_matrix.mean(axis=0)\n","#     reference_vectors[author] = mean_vector.tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# for author, ref_vec in reference_vectors.items():\n","#     max_value = max(ref_vec)\n","#     max_idx =ref_vec.index(max_value)\n","#     print(f'{author}: {words[max_idx]}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# authors"]},{"cell_type":"markdown","metadata":{"id":"3IoM1rGcdYT5","notebookRunGroups":{"groupValue":"1"}},"source":["# Check function in Test Data"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"expsystem","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
